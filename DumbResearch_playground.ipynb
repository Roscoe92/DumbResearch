{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "dcba66ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.webdriver as webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.core.os_manager import ChromeType\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "import subprocess\n",
    "import shutil\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from deutschland.bundesanzeiger import Bundesanzeiger\n",
    "\n",
    "\n",
    "def get_chromium_version() -> str:\n",
    "    try:\n",
    "        result = subprocess.run(['chromium', '--version'], capture_output=True, text=True)\n",
    "        version = result.stdout.split()[1]\n",
    "        return version\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def get_chromedriver_version() -> str:\n",
    "    try:\n",
    "        result = subprocess.run(['chromedriver', '--version'], capture_output=True, text=True)\n",
    "        version = result.stdout.split()[1]\n",
    "        return version\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def get_chromedriver_path() -> str:\n",
    "    return shutil.which('chromedriver')\n",
    "\n",
    "def get_webdriver_options(headless=False):\n",
    "    options = Options()\n",
    "    \n",
    "    if headless:\n",
    "        # The newer headless argument is \"--headless=new\" for Chrome 109+\n",
    "        # If you experience compatibility issues, fallback to \"--headless\"\n",
    "        options.add_argument(\"--headless=new\")\n",
    "\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--disable-features=NetworkService\")\n",
    "    options.add_argument(\"--window-size=1920x1080\")\n",
    "    options.add_argument(\"--disable-features=VizDisplayCompositor\")\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    return options\n",
    "\n",
    "def get_webdriver_service():\n",
    "    service = Service(\n",
    "        executable_path=get_chromedriver_path()\n",
    "    )\n",
    "    return service\n",
    "\n",
    "def run_selenium(domain, headless=False):\n",
    "    \"\"\"\n",
    "    Attempts to fetch the HTML content of `domain` using Selenium.\n",
    "    If headless=True, runs in headless mode; otherwise, opens a visible browser window.\n",
    "    \"\"\"\n",
    "    html_content = None\n",
    "\n",
    "    options = get_webdriver_options(headless=headless)\n",
    "    service = get_webdriver_service()\n",
    "\n",
    "    with webdriver.Chrome(options=options, service=service) as driver:\n",
    "        try:\n",
    "            driver.get(domain)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "            )\n",
    "            html_content = driver.page_source\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while fetching {domain} in headless={headless} mode: {e}\")\n",
    "\n",
    "    return html_content\n",
    "\n",
    "def extract_body_content(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    body_content = soup.body\n",
    "    if body_content:\n",
    "        return str(body_content)\n",
    "    return \"\"\n",
    "\n",
    "def clean_body_content(body_content):\n",
    "    soup = BeautifulSoup(body_content,'html.parser')\n",
    "\n",
    "    for script_or_style in soup(['script', 'style']):\n",
    "        script_or_style.extract()\n",
    "\n",
    "    cleaned_content = soup.get_text(separator='\\n')\n",
    "    cleaned_content = '\\n'.join(\n",
    "        line.strip() for line in cleaned_content.splitlines() if line.strip()\n",
    "        )\n",
    "    return cleaned_content\n",
    "\n",
    "def split_dom_content(dom_content, max_length = 8000):\n",
    "    return [\n",
    "        dom_content[i : i + max_length] for i in range(0,len(dom_content), max_length)\n",
    "    ]\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return all([parsed.scheme, parsed.netloc])  # Checks for scheme (http/https) and netloc (domain)\n",
    "\n",
    "\n",
    "def choose_links(links):\n",
    "    print(\"Here are the links to explore:\\n\")\n",
    "    for i, link in enumerate(links):\n",
    "        print(f\"{i}: {link}\")\n",
    "    \n",
    "    # Example input: \"0,2\"\n",
    "    user_input = input(\"\\nSelect the indexes of links you want to explore, separated by commas:\\n> \")\n",
    "    \n",
    "    # Convert the input string into a list of integers\n",
    "    indexes = [int(x.strip()) for x in user_input.split(\",\") if x.strip().isdigit()]\n",
    "    \n",
    "    chosen_links = [links[i] for i in indexes if i < len(links)]\n",
    "    return chosen_links\n",
    "\n",
    "\n",
    "def get_all_links(domain):\n",
    "    links_to_visit = []\n",
    "    try:\n",
    "        html = run_selenium(domain)\n",
    "\n",
    "        # If run_selenium returned None, bail out early:\n",
    "        if html is None:\n",
    "            print(f\"Error: No HTML returned from run_selenium for '{domain}'\")\n",
    "            return links_to_visit\n",
    "        \n",
    "        # save content\n",
    "        body_content = extract_body_content(html)  # Extract body content\n",
    "        cleaned_content = clean_body_content(body_content)  # Clean the content\n",
    "        \n",
    "        #pull respective links\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            # Resolve relative URLs\n",
    "            full_url = urljoin(domain, a_tag['href'])\n",
    "            links_to_visit.append(full_url)\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        # Refer to the domain (which is always defined), not full_url\n",
    "        print(f\"Error visiting domain '{domain}': {e}\")\n",
    "\n",
    "    return links_to_visit, cleaned_content\n",
    "\n",
    "def filter_urls_by_stem(stem_url, url_list):\n",
    "    \"\"\"\n",
    "    Compares each URL in url_list to stem_url.\n",
    "    If a URL starts with the stem_url, it's added to a matches list.\n",
    "    \n",
    "    :param stem_url: A string representing the 'stem' portion of the URL.\n",
    "    :param url_list: A list of full URLs to check against the stem URL.\n",
    "    :return: A list of URLs that match the stem URL.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for random_url in url_list:\n",
    "        # Simple check: does the random_url begin with stem_url?\n",
    "        if random_url.startswith(stem_url) and random_url not in matches and random_url != stem_url:\n",
    "            matches.append(random_url)\n",
    "    return matches\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_link(link, depth=5):\n",
    "    \"\"\"\n",
    "    Processes a single link by:\n",
    "      1) Getting its sub_links and sub_content\n",
    "      2) Filtering links based on the stem\n",
    "      3) Recursively expanding up to 'depth' levels\n",
    "      4) Returning a tuple: (link, link_set, content_dict_sub)\n",
    "    \"\"\"\n",
    "    # 1) Get sub_links and sub_content for this chosen link\n",
    "    sub_links, sub_content = get_all_links(link)\n",
    "\n",
    "    # Dictionary to store content for all pages under this link\n",
    "    content_dict_sub = {}\n",
    "    content_dict_sub[link] = sub_content\n",
    "\n",
    "    # Maintain a set of all discovered links for this domain\n",
    "    link_set = set()\n",
    "\n",
    "    # 2) Filter sub_links based on the stem\n",
    "    first_pass = filter_urls_by_stem(link, sub_links)\n",
    "    link_set.update(first_pass)\n",
    "\n",
    "    frontier = set(first_pass)\n",
    "\n",
    "    # 3) Expand up to specified depth\n",
    "    for depth_level in range(depth):\n",
    "        next_frontier = set()\n",
    "        for current_link in frontier:\n",
    "            current_links, current_content = get_all_links(current_link)\n",
    "            content_dict_sub[current_link] = current_content\n",
    "\n",
    "            new_links = set(filter_urls_by_stem(current_link, current_links))\n",
    "            new_links -= link_set\n",
    "\n",
    "            link_set.update(new_links)\n",
    "            next_frontier.update(new_links)\n",
    "\n",
    "        frontier = next_frontier\n",
    "\n",
    "    # 4) Return result: the original link, the aggregated link set, and all content\n",
    "    return (link, link_set, content_dict_sub)\n",
    "\n",
    "def preprocess(chosen_links, depth=5):\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    # We’ll store all the link expansions here\n",
    "    link_library = {}\n",
    "    # We’ll store the body/content here. Key = URL, Value = text content\n",
    "    content_dict_master = {}\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallelism\n",
    "    max_workers = min(8, len(chosen_links))  # Example: up to 8 threads or # of links\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit one process_link task per chosen link\n",
    "        future_to_link = {\n",
    "            executor.submit(process_link, link, depth): link\n",
    "            for link in chosen_links\n",
    "        }\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_link):\n",
    "            link = future_to_link[future]\n",
    "            try:\n",
    "                processed_link, link_set, content_dict_sub = future.result()\n",
    "\n",
    "                # Store the results in their respective data structures\n",
    "                link_library[processed_link] = list(link_set)\n",
    "                content_dict_master[processed_link] = content_dict_sub\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nPreprocess finished in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return link_library, content_dict_master\n",
    "\n",
    "def preprocess_content(content_dict):\n",
    "    master_dict = {}\n",
    "    for key in content_dict.keys():\n",
    "        master_dict[key] = ''.join(content_dict[key].values())\n",
    "    return master_dict\n",
    "\n",
    "def run_chatgpt(prompt, model = \"gpt-3.5-turbo\"):\n",
    "    client = OpenAI(\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),  # This is the default and can be omitted\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "       {\"role\": \"system\", \"content\": \"You are an advanced data extraction assistant.\"},\n",
    "       {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0  # For deterministic responses\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def first_pass_with_chatGPT(dom_chunks, parse_description):\n",
    "    \"\"\"\n",
    "    Sends chunks of website content to OpenAI's GPT API for parsing based on a description.\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop through chunks and get responses\n",
    "    parsed_results = []\n",
    "    for i, chunk in enumerate(dom_chunks, start=1):\n",
    "        template = (\n",
    "            f'You are tasked with extracting specific information from the following text content: {chunk}.'\n",
    "            'Please follow these instructions carefully: \\n\\n'\n",
    "            f' 1. ** Extract Information** : Only extract the information that directly matches the provided description {parse_description}'\n",
    "            ' 2. ** No Extra Content** : Do not include any additional text, comments or explanations in your response.'\n",
    "            ' 3. ** Empty Response** : If no information matches the description return to an empty string'\n",
    "            ' 4. ** Direct Data** : Your response should contain only data that is explicitly requested, with no other text.'\n",
    "            ' 5. ** Format** : **Single Table**: Output exactly one table, no extra lines or text.'\n",
    "                            '  - Use semicolons (`;`) to separate columns.'\n",
    "                            '  - Properly quote cells with (\"\") if they contain semicolons or commas.'\n",
    "                            '  - If a cell has no data, use `empty`.'\n",
    "            )\n",
    "        prompt = template\n",
    "        print(f\"Processing chunk {i}/{len(dom_chunks)}...\")\n",
    "\n",
    "        try:\n",
    "            response = run_chatgpt(prompt)\n",
    "            parsed_results.append(response.choices[0].message.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i}: {e}\")\n",
    "            parsed_results.append(\"\")  # Append an empty string if there's an error\n",
    "\n",
    "    initial_result = \"\\n\".join(parsed_results)\n",
    "\n",
    "    return initial_result\n",
    "\n",
    "def parse_semicolon_csv(llm_output, delimiters = None):\n",
    "    \"\"\"\n",
    "    Extracts table data from LLM output (potentially wrapped in ```csv code fences),\n",
    "    dynamically handles multiple delimiters, and ensures the resulting DataFrame has multiple columns.\n",
    "    \n",
    "    :param llm_output: The text output from the LLM containing tabular data.\n",
    "    :param delimiters: List of possible delimiters to handle, e.g., [\",\", \";\", \"\\t\", \"|\"].\n",
    "                       Defaults to common delimiters.\n",
    "    :return: A pandas DataFrame containing the parsed table data.\n",
    "    \"\"\"\n",
    "    # Default delimiters if none are provided\n",
    "    if delimiters is None:\n",
    "        delimiters = [\",\", \";\", \"\\t\", \"|\"]\n",
    "    \n",
    "    # 1) Extract the CSV text from code fences, if present\n",
    "    pattern = r\"```csv\\n(.*?)```\"\n",
    "    match = re.search(pattern, llm_output, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        csv_text = match.group(1).strip()\n",
    "    else:\n",
    "        # If no fenced block found, assume entire output is the CSV\n",
    "        csv_text = llm_output.strip()\n",
    "\n",
    "    # 2) Basic cleanup: remove trailing spaces or lines\n",
    "    lines = [line.strip() for line in csv_text.splitlines() if line.strip()]\n",
    "    csv_text = \"\\n\".join(lines)\n",
    "\n",
    "    # 3) Try parsing the CSV with each delimiter\n",
    "    for delimiter in delimiters:\n",
    "        try:\n",
    "            # Attempt to parse the text into a DataFrame\n",
    "            df = pd.read_csv(\n",
    "                StringIO(csv_text),\n",
    "                sep=delimiter,\n",
    "                engine=\"python\",\n",
    "                quotechar='\"',\n",
    "                on_bad_lines=\"skip\"  # Skip lines that don't match columns\n",
    "            )\n",
    "            # Check if the resulting DataFrame has more than one column\n",
    "            if df.shape[1] > 1:\n",
    "                break\n",
    "        except Exception:\n",
    "            # Ignore errors and try the next delimiter\n",
    "            continue\n",
    "    else:\n",
    "        # If none of the delimiters work, raise an error\n",
    "        raise ValueError(\n",
    "            \"Could not parse the table data with any of the specified delimiters or the result only has one column.\"\n",
    "        )\n",
    "    \n",
    "    # 4) Remove empty and unnamed columns\n",
    "    df.dropna(axis=\"columns\", how=\"all\", inplace=True)\n",
    "    df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "    # 5) Clean column names\n",
    "    df.columns = [col.strip('\" ').strip() for col in df.columns]\n",
    "\n",
    "    # 6) Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def scrape_to_df(content_dict,parse_description):\n",
    "    start_time = time.time()\n",
    "    outputs = {}\n",
    "    master_dict = preprocess_content(content_dict)\n",
    "    counter = 0\n",
    "    for key in master_dict:\n",
    "        counter += 1\n",
    "        print(f'Processing {key}, {counter} out of {len(master_dict.keys())} datasets to process.')\n",
    "        dom_content = master_dict[key]\n",
    "        dom_chunks = split_dom_content(dom_content)\n",
    "        csv_output = first_pass_with_chatGPT(dom_chunks,parse_description)\n",
    "        outputs[key] = parse_semicolon_csv(csv_output)\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nParsing scrape results finished in {end_time - start_time:.2f} seconds\")\n",
    "    return outputs\n",
    "\n",
    "def run_research(website,parse_description):\n",
    "    links,content = preprocess(website)\n",
    "    output = scrape_to_df(content,parse_description)\n",
    "    return output\n",
    "\n",
    "def select_topics():\n",
    "    def is_valid_url(url):\n",
    "        pattern = re.compile(r'^(https?:\\/\\/)?([\\w.-]+)+(:\\d+)?(\\/[\\w._-]*)*\\/?$')\n",
    "        return re.match(pattern, url)\n",
    "\n",
    "    website_input = input(\"\\nPlease paste the websites you'd like to explore, separated by commas: \")\n",
    "    if not website_input.strip():\n",
    "        print(\"No websites entered. Please try again.\")\n",
    "        return []\n",
    "\n",
    "    websites = [str(x.strip().lower().rstrip(\"/\")) for x in website_input.split(\",\") if x.strip()]\n",
    "    valid_websites = [x for x in websites if is_valid_url(x)]\n",
    "\n",
    "    if len(valid_websites) == 0:\n",
    "        print(\"No valid websites provided.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Websites selected: {', '.join(valid_websites)}\")\n",
    "    return valid_websites\n",
    "\n",
    "def user_selects(players = None):\n",
    "    if players is None:\n",
    "        domains = select_topics()\n",
    "    else:\n",
    "        domains = players.tolist() if isinstance(players, pd.Series) else players\n",
    "    domain_links = {}\n",
    "    for domain in domains:\n",
    "        initial_links, initial_content = get_all_links(domain)\n",
    "        domain_links[domain] = choose_links(initial_links)\n",
    "    return domain_links\n",
    "\n",
    "def user_task():\n",
    "    user_input = input('\\nPlease let DumbResearch know:\\n1. What web content it is looking at \\n2. What columns you would like your research output table to have \\n\\nSee example below: \\n\"These are the solutions offered by a german cyber security player (input is provided in German), please provide a table or tables that list the respective products / solutions with important information on the respective products e.g. their descriptions, features, target customers, KPIs\"\\n')\n",
    "    return user_input\n",
    "\n",
    "def initialize_dumbresearch(players = None):\n",
    "    if players is not None and not isinstance(players, (list, pd.Series)):\n",
    "        raise ValueError(\"`players` must be a list or Pandas Series of URLs\")\n",
    "    domains_selected = user_selects(players)\n",
    "    parse_description = user_task()\n",
    "    return domains_selected, parse_description\n",
    "\n",
    "def dumbresearch_workflow(players = None):\n",
    "    domains, parse_description = initialize_dumbresearch(players)\n",
    "    print(domains)\n",
    "    outputs = {}\n",
    "    for key in domains.keys():\n",
    "        links, content = preprocess(domains[key])\n",
    "        output = scrape_to_df(content, parse_description)\n",
    "        outputs[key] = output\n",
    "    return outputs\n",
    "\n",
    "def find_competitors():\n",
    "    company = input('\\n**Target**\\nPlease let DumbResearch know, on which company you are conducting desktop research')\n",
    "    company_context = input('\\n**Target description**\\nPlease let DumbResearch know more about the company, i.e. what industry, geographies and if you want it to pay particular attention to anything or exclude something in its research!\\n')\n",
    "    result = parse_semicolon_csv(competitors_with_chatGPT(company,company_context))\n",
    "    return result\n",
    "\n",
    "\n",
    "def competitors_with_chatGPT(company, company_context):\n",
    "    \"\"\"\n",
    "    Sends chunks of website content to OpenAI's GPT API for parsing based on a description.\n",
    "    \"\"\"\n",
    "\n",
    "    # Loop through chunks and get responses\n",
    "    parsed_results = []\n",
    "    template = (\n",
    "        f'You are tasked with finding the competitors of a given company called {company}, with the below context provided: {company_context}.'\n",
    "        'Please follow these instructions carefully: \\n\\n'\n",
    "        ' 1. ** Extract Information** : Only extract the information that directly matches the provided description above'\n",
    "        ' 2. ** Volume** : Find at least 10 competitors for each requests.'\n",
    "        ' 3. ** Empty Response** : If no information matches the description return to an empty string'\n",
    "        ' 4. ** Direct Data** : Your response should contain only data that is explicitly requested, with no other text.'\n",
    "        ' 5. ** Format** : **Single Table**: Output exactly one table, no extra lines or text.'\n",
    "                        '  - Use semicolons (`;`) to separate columns.'\n",
    "                        '  - Properly quote cells with (\"\") if they contain semicolons or commas.'\n",
    "                        '  - If a cell has no data, use `empty`.'\n",
    "                        '  - Columns are the 1) respective competitors (\"Competitor\") 2) the industry they are in (\"Industry\") 3) the link to their website (\"Website\") 4) The name of the legal entity that is registered with the authorities (\"Legal entity\")'\n",
    "        )\n",
    "    prompt = template\n",
    "    print(f\"Finding competitors of {company}...\")\n",
    "\n",
    "    try:\n",
    "        response = run_chatgpt(prompt, model = 'gpt-4o')\n",
    "        parsed_results.append(response.choices[0].message.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {i}: {e}\")\n",
    "        parsed_results.append(\"\")  # Append an empty string if there's an error\n",
    "\n",
    "    initial_result = \"\\n\".join(parsed_results)\n",
    "\n",
    "    return initial_result\n",
    "\n",
    "def competitor_search_workflow():\n",
    "    result = find_competitors()\n",
    "    \n",
    "    # Display found competitors\n",
    "    for i, link in enumerate(result['Website']):\n",
    "        print(f\"{i}: {link}\")\n",
    "        \n",
    "    # Ask the user for additional competitors\n",
    "    user_input = input(\n",
    "        \"\\nThese are the competitors DumbResearch found. \"\n",
    "        \"Do you have any additional players you would like to add to the search? \"\n",
    "        \"Please add their respective websites, separated by a comma:\\n\"\n",
    "    )\n",
    "    add_players = [str(x.strip().lower().rstrip(\"/\")) for x in user_input.split(\",\") if x.strip()]\n",
    "    \n",
    "    # Add new competitors to the result DataFrame\n",
    "    for i in add_players:\n",
    "        new_row = {'Competitor': None, 'Industry': None, 'Website': i, 'Legal entity': None}\n",
    "        result = pd.concat([result, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    \n",
    "    # Display the updated list of competitors\n",
    "    for i, link in enumerate(result['Website']):\n",
    "        print(f\"{i}: {link}\")\n",
    "    \n",
    "    # Ask the user to select competitors to keep\n",
    "    user_input = input(\n",
    "        \"\\nThis is the current set of competitors. \"\n",
    "        \"Select the indexes of competitors you want to keep, separated by commas:\\n> \"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Convert the input string into a list of integers\n",
    "        indexes = [int(x.strip()) for x in user_input.split(\",\") if x.strip().isdigit()]\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a comma-separated list of numbers.\")\n",
    "        return {}\n",
    "    \n",
    "    # Validate indexes and filter the selected competitors\n",
    "    chosen_competitors = result.iloc[indexes].reset_index(drop=True) if indexes else pd.DataFrame()\n",
    "    \n",
    "    return chosen_competitors\n",
    "\n",
    "def search_research_workflow():\n",
    "    competitor_set = competitor_search_workflow()\n",
    "    print(f\"Now looking at the websites of your competitor set {competitor_set['Website']}\")\n",
    "    output = dumbresearch_workflow(competitor_set['Website'])\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def parse_semicolon_csv_to_dataframe(csv_text):\n",
    "    \"\"\"\n",
    "    Dynamically determine the max number of columns across all lines\n",
    "    and use that to build the DataFrame.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in csv_text.strip().splitlines() if line.strip()]\n",
    "    if not lines:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Split everything first\n",
    "    splitted_lines = [row.split(\";\") for row in lines]\n",
    "\n",
    "    # Find the maximum column count\n",
    "    max_cols = max(len(sl) for sl in splitted_lines)\n",
    "\n",
    "    # First row is still used as headers, but it might not have all columns\n",
    "    headers = splitted_lines[0]\n",
    "    if len(headers) < max_cols:\n",
    "        # Pad the header row with \"ColumnX\" or \"empty\" for missing columns\n",
    "        missing_count = max_cols - len(headers)\n",
    "        headers += [f\"ExtraCol{i+1}\" for i in range(missing_count)]\n",
    "    elif len(headers) > max_cols:\n",
    "        # If the header has more columns than any data row, just truncate it\n",
    "        headers = headers[:max_cols]\n",
    "\n",
    "    # For all subsequent rows, pad or truncate\n",
    "    data_rows = []\n",
    "    for row in splitted_lines[1:]:\n",
    "        if len(row) < max_cols:\n",
    "            row += [\"empty\"] * (max_cols - len(row))\n",
    "        elif len(row) > max_cols:\n",
    "            row = row[:max_cols]\n",
    "        data_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data_rows, columns=headers)\n",
    "    return df\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 2) Individual Extraction Functions\n",
    "#############################################\n",
    "\n",
    "def extract_income_statement(report_text):\n",
    "    \"\"\"\n",
    "    Extracts the Income Statement from the given `report_text`.\n",
    "    Returns a DataFrame, or empty DataFrame if none found.\n",
    "    \"\"\"\n",
    "    # Single-task instructions to ChatGPT\n",
    "    income_statement_instructions = \"\"\"\n",
    "    You are tasked with extracting only the Income Statement data\n",
    "    from the following report. The output should be formatted as\n",
    "    a semicolon-delimited table with a header row.\n",
    "\n",
    "    Instructions:\n",
    "    1. Only output the Income Statement (no other tables).\n",
    "    2. Use semicolons (;) to separate columns.\n",
    "    3. If certain cells are empty, write 'empty'.\n",
    "    4. Do not include extra commentary or text outside the table.\n",
    "    5. The first line should be the header row.\n",
    "\n",
    "    Here is the input:\n",
    "    {report_text}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = income_statement_instructions.format(report_text=report_text)\n",
    "    response_text = run_chatgpt(prompt)\n",
    "    response_text = response_text.choices[0].message.content\n",
    "    df = parse_semicolon_csv_to_dataframe(response_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_operational_kpis(report_text):\n",
    "    \"\"\"\n",
    "    Extracts important operational KPIs from the report text,\n",
    "    such as production volume, customer count, churn rate, ARPU, etc.\n",
    "    Returns a DataFrame.\n",
    "    \"\"\"\n",
    "    kpi_instructions = \"\"\"\n",
    "    You are tasked with extracting only the operational KPIs from\n",
    "    the following report. Examples of such KPIs include:\n",
    "    - Number of customers or clients\n",
    "    - Churn rate\n",
    "    - ARPU (Average Revenue per User)\n",
    "    - Employee headcount\n",
    "    - Production volume\n",
    "    - Utilization rates\n",
    "    - Other similar quantitative metrics\n",
    "\n",
    "    Output a semicolon-delimited table with a header row.\n",
    "    Each row should contain:\n",
    "       - KPI Name\n",
    "       - KPI Value\n",
    "       - (Optionally) a KPI Description\n",
    "\n",
    "    Only output this single table, no other text.\n",
    "\n",
    "    Here is the input:\n",
    "    {report_text}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = kpi_instructions.format(report_text=report_text)\n",
    "    response_text = run_chatgpt(prompt)\n",
    "    response_text = response_text.choices[0].message.content\n",
    "    df = parse_semicolon_csv_to_dataframe(response_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_qualitative_info(report_text):\n",
    "    \"\"\"\n",
    "    Extracts only qualitative information, e.g.,\n",
    "    management commentary, forward-looking statements, strategic objectives, etc.\n",
    "    Returns a DataFrame with two columns: label, content.\n",
    "    \"\"\"\n",
    "    qualitative_instructions = \"\"\"\n",
    "    You are tasked with extracting only qualitative information\n",
    "    such as management commentary, forward-looking statements,\n",
    "    strategic objectives, or other narrative insights from the\n",
    "    following report. Each piece of qualitative info should be\n",
    "    categorized under a 'label', with the actual commentary in 'content'.\n",
    "\n",
    "    Output a semicolon-delimited table with this header:\n",
    "    \"label\";\"content\"\n",
    "\n",
    "    Only output this single table, no extra text or lines.\n",
    "\n",
    "    Here is the input:\n",
    "    {report_text}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = qualitative_instructions.format(report_text=report_text)\n",
    "    response_text = run_chatgpt(prompt)\n",
    "    response_text = response_text.choices[0].message.content\n",
    "    df = parse_semicolon_csv_to_dataframe(response_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 3) Orchestrator for a Single Report\n",
    "#############################################\n",
    "\n",
    "def parse_single_report(report_data):\n",
    "    \"\"\"\n",
    "    Given one report (dictionary with keys like date, company, report),\n",
    "    runs the three extraction functions and returns a dictionary of DataFrames.\n",
    "    e.g. {\n",
    "        \"Income Statement\": <DataFrame>,\n",
    "        \"KPIs\": <DataFrame>,\n",
    "        \"Qualitative\": <DataFrame>\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    report_text = report_data.get(\"report\", \"\")\n",
    "    if not report_text:\n",
    "        # Return empty placeholders if no text\n",
    "        return {\n",
    "            \"Income Statement\": pd.DataFrame(),\n",
    "            \"KPIs\": pd.DataFrame(),\n",
    "            \"Qualitative\": pd.DataFrame()\n",
    "        }\n",
    "\n",
    "    # Calls each extraction function separately\n",
    "    income_df = extract_income_statement(report_text)\n",
    "    kpi_df = extract_operational_kpis(report_text)\n",
    "    qual_df = extract_qualitative_info(report_text)\n",
    "\n",
    "    return {\n",
    "        \"Income Statement\": income_df,\n",
    "        \"KPIs\": kpi_df,\n",
    "        \"Qualitative\": qual_df\n",
    "    }\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 4) parse_bundesanzeiger_reports\n",
    "#############################################\n",
    "\n",
    "def parse_bundesanzeiger_reports(reports_dict):\n",
    "    \"\"\"\n",
    "    Takes a dictionary of reports, keyed by unique IDs, where each value \n",
    "    is a dictionary containing 'date', 'company', 'report' text, etc.\n",
    "\n",
    "    Returns a final structure:\n",
    "    {\n",
    "        <company_name>: {\n",
    "            <year>: {\n",
    "                <report_id>: {\n",
    "                   \"Income Statement\": DataFrame,\n",
    "                   \"KPIs\": DataFrame,\n",
    "                   \"Qualitative\": DataFrame\n",
    "                },\n",
    "                ...\n",
    "            },\n",
    "            ...\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    master_outputs = {}\n",
    "\n",
    "    for report_id, report_data in reports_dict.items():\n",
    "        # Identify the company\n",
    "        company_name = report_data.get(\"company\", \"Unknown Company\")\n",
    "\n",
    "        # Parse out the year from the date\n",
    "        raw_date = report_data.get(\"date\", None)\n",
    "        parsed_year = None\n",
    "        if isinstance(raw_date, datetime.date):\n",
    "            parsed_year = raw_date.year\n",
    "        elif isinstance(raw_date, str):\n",
    "            try:\n",
    "                dt = datetime.datetime.strptime(raw_date, \"%Y-%m-%d\")\n",
    "                parsed_year = dt.year\n",
    "            except ValueError:\n",
    "                print(f\"Could not parse date for report {report_id}: {raw_date}\")\n",
    "        if not parsed_year:\n",
    "            print(f\"Skipping report {report_id}: No valid year.\")\n",
    "            continue\n",
    "\n",
    "        # Build structure: master_outputs[company][year][report_id]\n",
    "        if company_name not in master_outputs:\n",
    "            master_outputs[company_name] = {}\n",
    "        if parsed_year not in master_outputs[company_name]:\n",
    "            master_outputs[company_name][parsed_year] = {}\n",
    "\n",
    "        # Parse the single report using the orchestrator function\n",
    "        results_for_this_report = parse_single_report(report_data)\n",
    "\n",
    "        # Save the results\n",
    "        master_outputs[company_name][parsed_year][report_id] = results_for_this_report\n",
    "\n",
    "    return master_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "b45e9b67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocess finished in 8.70 seconds\n"
     ]
    }
   ],
   "source": [
    "result = preprocess(['https://www.stackinfra.com/solutions/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "6c14bafe-055c-4ae9-8921-9fdf4dcec0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 19:21:39.787 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:21:41.073 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:21:46.391 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:21:48.798 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:21:50.081 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:21:54.122 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:21:59.794 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:00.352 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:04.821 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:06.652 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:08.151 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:10.974 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:12.553 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:13.681 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:18.493 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:28.849 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:29.757 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:33.743 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:36.478 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:37.608 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:42.353 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:43.480 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:45.247 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:48.907 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:51.004 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:51.798 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:55.459 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:56.799 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:57.353 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:22:59.253 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:23:00.573 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:23:01.167 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:23:05.411 INFO    httpx: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "reports = parse_bundesanzeiger_reports(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "4fcef7b5-79d7-4591-a487-66ba1c80438a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2023, 2021, 2020, 2019, 2017, 2016, 2015, 2014])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports['Myra Security GmbH'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "362dfad5-2b81-49e4-8dbb-9d4e5bded9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "api_key = os.getenv(\"COMPANIES_HOUSE_API_KEY\")\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_company_number(company_name, api_key):\n",
    "    \"\"\"\n",
    "    Fetches the company number of the first search result for a given company name.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to search for.\n",
    "        api_key (str): Your Companies House API key.\n",
    "\n",
    "    Returns:\n",
    "        str: The company number of the first search result.\n",
    "        None: If no company is found or an error occurs.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.company-information.service.gov.uk\"\n",
    "    search_url = f\"{base_url}/search/companies?q={company_name}\"\n",
    "\n",
    "    # Call the search endpoint\n",
    "    response = requests.get(search_url, auth=(api_key, ''))\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Search request failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    # Check if the search returned any results\n",
    "    if \"items\" not in data or len(data[\"items\"]) == 0:\n",
    "        print(f\"No results found for company name: {company_name}\")\n",
    "        return None\n",
    "\n",
    "    # Extract the company number of the first result\n",
    "    company_number = data[\"items\"][0][\"company_number\"]\n",
    "    return company_number\n",
    "\n",
    "def get_filing_history(company_number, api_key):\n",
    "    \"\"\"\n",
    "    Fetches the complete filing history of a company using its company number,\n",
    "    handling pagination to collect all items.\n",
    "\n",
    "    Args:\n",
    "        company_number (str): The unique company number of the company.\n",
    "        api_key (str): Your Companies House API key.\n",
    "\n",
    "    Returns:\n",
    "        list: A complete list of filing history items.\n",
    "        None: If no filing history is found or an error occurs.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.company-information.service.gov.uk\"\n",
    "    filing_history_url = f\"{base_url}/company/{company_number}/filing-history\"\n",
    "\n",
    "    all_items = []  # List to store all filing history items\n",
    "    start_index = 0  # Initial page index\n",
    "    items_per_page = 25  # Default items per page (as per Companies House API documentation)\n",
    "\n",
    "    while True:\n",
    "        # Add pagination parameters to the request\n",
    "        params = {\n",
    "            \"start_index\": start_index,\n",
    "            \"items_per_page\": items_per_page\n",
    "        }\n",
    "\n",
    "        # Call the filing history endpoint\n",
    "        response = requests.get(filing_history_url, params=params, auth=(api_key, ''))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Filing history request failed with status code {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Append the items from the current page to the list\n",
    "        items = data.get(\"items\", [])\n",
    "        all_items.extend(items)\n",
    "\n",
    "        # Check if we have fetched all items\n",
    "        total_count = data.get(\"total_count\", 0)\n",
    "        if len(all_items) >= total_count:\n",
    "            break\n",
    "\n",
    "        # Update the start_index for the next page\n",
    "        start_index += items_per_page\n",
    "\n",
    "    return all_items\n",
    "    \n",
    "def get_filing_history(company_number, api_key):\n",
    "    \"\"\"\n",
    "    Fetches the complete filing history of a company using its company number,\n",
    "    handling pagination to collect all items.\n",
    "\n",
    "    Args:\n",
    "        company_number (str): The unique company number of the company.\n",
    "        api_key (str): Your Companies House API key.\n",
    "\n",
    "    Returns:\n",
    "        list: A complete list of filing history items.\n",
    "        None: If no filing history is found or an error occurs.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.company-information.service.gov.uk\"\n",
    "    filing_history_url = f\"{base_url}/company/{company_number}/filing-history\"\n",
    "\n",
    "    all_items = []  # List to store all filing history items\n",
    "    start_index = 0  # Initial page index\n",
    "    items_per_page = 25  # Default items per page (as per Companies House API documentation)\n",
    "\n",
    "    while True:\n",
    "        # Add pagination parameters to the request\n",
    "        params = {\n",
    "            \"start_index\": start_index,\n",
    "            \"items_per_page\": items_per_page\n",
    "        }\n",
    "\n",
    "        # Call the filing history endpoint\n",
    "        response = requests.get(filing_history_url, params=params, auth=(api_key, ''))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Filing history request failed with status code {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Append the items from the current page to the list\n",
    "        items = data.get(\"items\", [])\n",
    "        all_items.extend(items)\n",
    "\n",
    "        # Check if we have fetched all items\n",
    "        total_count = data.get(\"total_count\", 0)\n",
    "        if len(all_items) >= total_count:\n",
    "            break\n",
    "\n",
    "        # Update the start_index for the next page\n",
    "        start_index += items_per_page\n",
    "\n",
    "    return all_items\n",
    "\n",
    "\n",
    "def get_company_filing_history(company_name, api_key):\n",
    "    \"\"\"\n",
    "    Fetches the filing history of a company using its name.\n",
    "\n",
    "    Args:\n",
    "        company_name (str): The name of the company to search for.\n",
    "        api_key (str): Your Companies House API key.\n",
    "\n",
    "    Returns:\n",
    "        dict: Filing history details or an error message.\n",
    "    \"\"\"\n",
    "    # Step 1: Get the company number\n",
    "    company_number = get_company_number(company_name, api_key)\n",
    "\n",
    "    if not company_number:\n",
    "        return {\"error\": f\"Could not find company number for: {company_name}\"}\n",
    "\n",
    "    # Step 2: Get the filing history\n",
    "    filing_history = get_filing_history(company_number, api_key)\n",
    "\n",
    "    if not filing_history:\n",
    "        return {\"error\": f\"Could not retrieve filing history for company number: {company_number}\"}\n",
    "\n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_number\": company_number,\n",
    "        \"filing_history\": filing_history,\n",
    "    }\n",
    "\n",
    "import requests\n",
    "\n",
    "import os\n",
    "\n",
    "def fetch_document(document_id, api_key, output_path):\n",
    "    \"\"\"\n",
    "    Fetches a document from the Companies House Document API and saves it locally.\n",
    "\n",
    "    Args:\n",
    "        document_id (str): The ID of the document to fetch.\n",
    "        api_key (str): Your Companies House API key.\n",
    "        output_path (str): The file path (or directory) where the document will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the document was successfully fetched and saved, False otherwise.\n",
    "    \"\"\"\n",
    "    # If the output path is a directory, append a default filename\n",
    "    if os.path.isdir(output_path):\n",
    "        output_path = os.path.join(output_path, f\"{document_id}.pdf\")\n",
    "\n",
    "    base_url = \"https://document-api.company-information.service.gov.uk\"\n",
    "    document_url = f\"{base_url}/document/{document_id}/content\"\n",
    "\n",
    "    # Headers for the request\n",
    "    headers = {\n",
    "        \"Accept\": \"application/pdf\",  # Request the document as a PDF\n",
    "    }\n",
    "\n",
    "    # Send the request to fetch the document\n",
    "    response = requests.get(document_url, headers=headers, auth=(api_key, ''), allow_redirects=False)\n",
    "\n",
    "    if response.status_code == 302:  # The document is available\n",
    "        # Follow the redirect URL in the 'Location' header to download the document\n",
    "        redirect_url = response.headers.get(\"Location\")\n",
    "        if not redirect_url:\n",
    "            print(f\"Error: Redirect URL not provided for document_id {document_id}.\")\n",
    "            return False\n",
    "\n",
    "        # Fetch the document content\n",
    "        document_response = requests.get(redirect_url)\n",
    "\n",
    "        if document_response.status_code == 200:\n",
    "            # Save the document locally\n",
    "            with open(output_path, \"wb\") as file:\n",
    "                file.write(document_response.content)\n",
    "            print(f\"Document {document_id} saved to {output_path}.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error: Failed to fetch document content. Status code {document_response.status_code}.\")\n",
    "            return False\n",
    "\n",
    "    elif response.status_code == 401:\n",
    "        print(\"Error: Unauthorized. Check your API key.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Error: Failed to fetch document. Status code {response.status_code}.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "209bdb11-8e54-4464-8af4-1c7c623791fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = 'G.Network Communications Ltd'\n",
    "\n",
    "os.environ[\"COMPANIES_HOUSE_API_KEY\"] = \"cd88de94-9a32-496c-8970-bdaa3ec3d61c\"\n",
    "\n",
    "api_key = os.getenv(\"COMPANIES_HOUSE_API_KEY\")\n",
    "\n",
    "result = get_company_filing_history(company_name,api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "1a8f3028-7d36-4133-9d5c-30373f78caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document dXXqzTVwa0UWdU-WTcDHaSuW0wIxOZc4flgHPf4isdk saved to /Users/roscoe/Downloads/dXXqzTVwa0UWdU-WTcDHaSuW0wIxOZc4flgHPf4isdk.pdf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/roscoe/Downloads'\n",
    "doc_id = 'dXXqzTVwa0UWdU-WTcDHaSuW0wIxOZc4flgHPf4isdk'\n",
    "fetch_document(doc_id, api_key, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "3bbed3b1-454f-4fa6-8131-60d067a9f565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transaction_id': 'MzQzMTUxNTg3OWFkaXF6a2N4', 'barcode': 'AD8PZ9A8', 'type': 'AA', 'date': '2024-08-13', 'category': 'accounts', 'description': 'accounts-with-accounts-type-group', 'description_values': {'made_up_date': '2024-03-31'}, 'pages': 65, 'action_date': '2024-03-31', 'paper_filed': True, 'links': {'self': '/company/10057745/filing-history/MzQzMTUxNTg3OWFkaXF6a2N4', 'document_metadata': 'https://document-api.company-information.service.gov.uk/document/dXXqzTVwa0UWdU-WTcDHaSuW0wIxOZc4flgHPf4isdk'}}\n",
      "{'transaction_id': 'MzQwNDYxMTMwMmFkaXF6a2N4', 'barcode': 'BCIEMDJM', 'type': 'AA', 'date': '2023-12-28', 'category': 'accounts', 'description': 'accounts-with-accounts-type-group', 'description_values': {'made_up_date': '2023-03-31'}, 'pages': 58, 'action_date': '2023-03-31', 'paper_filed': True, 'links': {'self': '/company/10057745/filing-history/MzQwNDYxMTMwMmFkaXF6a2N4', 'document_metadata': 'https://document-api.company-information.service.gov.uk/document/mGTBsga6S0Z8l3XM8NegVKaLjyyK0AkH_AClUinO0GQ'}}\n",
      "{'transaction_id': 'MzM1NDA3MDMwOWFkaXF6a2N4', 'barcode': 'ABDUWUND', 'type': 'AA', 'date': '2022-10-10', 'category': 'accounts', 'description': 'accounts-with-accounts-type-group', 'description_values': {'made_up_date': '2022-03-31'}, 'pages': 62, 'action_date': '2022-03-31', 'paper_filed': True, 'links': {'self': '/company/10057745/filing-history/MzM1NDA3MDMwOWFkaXF6a2N4', 'document_metadata': 'https://document-api.company-information.service.gov.uk/document/7aofdYOmoskKigq0znJvEMpjqu_7swuH3nipl8msEcg'}}\n"
     ]
    }
   ],
   "source": [
    "for report in result['filing_history']:\n",
    "    if report['description'] == 'accounts-with-accounts-type-group':\n",
    "        print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "7ac2a105-7a43-409a-b5b9-e5d1ac9e585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_full_report(data):\n",
    "    \"\"\"\n",
    "    Parse a financial report, preserving both quantitative and qualitative information.\n",
    "    \n",
    "    Args:\n",
    "    - data (dict): A dictionary containing 'date', 'name', 'company', 'report', and other metadata.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A structured dictionary with parsed financial and qualitative information.\n",
    "    \"\"\"\n",
    "    report = data.get(\"report\", \"\")\n",
    "    \n",
    "    # Extract specific sections using delimiters\n",
    "    aktiva_match = re.search(r\"AKTIVA(.*?)PASSIVA\", report, re.DOTALL)\n",
    "    passiva_match = re.search(r\"PASSIVA(.*?)Allgemeine Angaben\", report, re.DOTALL)\n",
    "    general_info_match = re.search(r\"Allgemeine Angaben(.*?)Angaben zur Bilanz\", report, re.DOTALL)\n",
    "    bilanzi_info_match = re.search(r\"Angaben zur Bilanz(.*?)Haftungsverhältnisse\", report, re.DOTALL)\n",
    "    employee_program_match = re.search(r\"Virtuelles Mitarbeiterprogramm(.*?)Sonstige finanzielle Verpflichtungen\", report, re.DOTALL)\n",
    "    obligations_match = re.search(r\"Sonstige finanzielle Verpflichtungen(.*?)München,\", report, re.DOTALL)\n",
    "    \n",
    "    aktiva_data = aktiva_match.group(1).strip() if aktiva_match else \"\"\n",
    "    passiva_data = passiva_match.group(1).strip() if passiva_match else \"\"\n",
    "    general_info = general_info_match.group(1).strip() if general_info_match else \"\"\n",
    "    bilanzi_info = bilanzi_info_match.group(1).strip() if bilanzi_info_match else \"\"\n",
    "    employee_program = employee_program_match.group(1).strip() if employee_program_match else \"\"\n",
    "    obligations = obligations_match.group(1).strip() if obligations_match else \"\"\n",
    "    \n",
    "    def parse_section(section):\n",
    "        \"\"\"\n",
    "        Parse a financial section (e.g., AKTIVA or PASSIVA) into a structured list of items.\n",
    "        \"\"\"\n",
    "        items = []\n",
    "        lines = section.split(\"\\n\")\n",
    "        \n",
    "        for line in lines:\n",
    "            match = re.match(r\"(.*?)(\\d{1,3}(?:\\.\\d{3})*,\\d{2})(?:\\s+(\\d{1,3}(?:\\.\\d{3})*,\\d{2}))?\", line.strip())\n",
    "            if match:\n",
    "                item = match.group(1).strip()\n",
    "                current_year = match.group(2).replace(\".\", \"\").replace(\",\", \".\")\n",
    "                previous_year = match.group(3).replace(\".\", \"\").replace(\",\", \".\") if match.group(3) else None\n",
    "                items.append({\n",
    "                    \"Item\": item,\n",
    "                    \"Current Year (EUR)\": float(current_year),\n",
    "                    \"Previous Year (EUR)\": float(previous_year) if previous_year else None\n",
    "                })\n",
    "        return items\n",
    "    \n",
    "    # Parse AKTIVA and PASSIVA sections\n",
    "    aktiva = parse_section(aktiva_data)\n",
    "    passiva = parse_section(passiva_data)\n",
    "    \n",
    "    # Build the final structured output\n",
    "    parsed_data = {\n",
    "        \"Metadata\": {\n",
    "            \"Date\": data.get(\"date\"),\n",
    "            \"Company\": data.get(\"company\"),\n",
    "            \"Report Name\": data.get(\"name\"),\n",
    "        },\n",
    "        \"Financial Data\": {\n",
    "            \"AKTIVA\": aktiva,\n",
    "            \"PASSIVA\": passiva,\n",
    "        },\n",
    "        \"Qualitative Information\": {\n",
    "            \"General Info\": general_info,\n",
    "            \"Balance Sheet Notes\": bilanzi_info,\n",
    "            \"Employee Program\": employee_program,\n",
    "            \"Financial Obligations\": obligations,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "bcc47bc2-37ce-4070-8ce8-f418652d812b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': datetime.datetime(2023, 7, 3, 0, 0),\n",
       " 'name': 'Jahresabschluss zum Geschäftsjahr vom 01.01.2021 bis zum 31.12.2021',\n",
       " 'company': 'Myra Security GmbH',\n",
       " 'report': '\\n\\n\\n\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\nMyra Security GmbH\\nMünchen\\nJahresabschluss zum Geschäftsjahr vom 01.01.2021 bis zum 31.12.2021\\nBILANZ zum 31. Dezember 2021\\n\\nAKTIVA \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGeschäftsjahr\\nVorjahr\\n\\n\\n\\n\\nEUR\\nEUR\\nEUR\\n\\n\\n\\n\\nA. Anlagevermögen\\n\\n\\n\\n\\n\\n\\n\\n\\nI. Immaterielle Vermögensgegenstände\\n399.321,07\\n\\n\\n229.063,00\\n\\n\\nII. Sachanlagen \\n935.797,80\\n\\n\\n709.159,05\\n\\n\\n\\n\\n\\n\\n1.335.118,87\\n938.222,05\\n\\n\\nB. Umlaufvermögen\\n\\n\\n\\n\\n\\n\\n\\n\\nI. Vorräte\\n51.625,54\\n\\n\\n0,00\\n\\n\\nII. Forderungen und sonstige Vermögensgegenstände\\n886.016,79\\n\\n\\n557.204,85\\n\\n\\nIII. Kassenbestand,\\n                     Bundesbankguthaben, Guthaben bei\\n                     Kreditinstituten und Schecks\\n2.285.848,38\\n\\n\\n4.994.256,82\\n\\n\\n\\n\\n\\n\\n3.223.490,71\\n5.551.461,67\\n\\n\\nC. Rechnungsabgrenzungsposten\\n\\n\\n98.740,46\\n56.826,34\\n\\n\\n\\n\\n\\n\\n4.657.350,04\\n6.546.510,06\\n\\n\\n\\n\\nPASSIVA\\n               \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGeschäftsjahr\\nVorjahr\\n\\n\\n\\n\\nEUR\\nEUR\\nEUR\\n\\n\\n\\n\\nA. Eigenkapital\\n                     \\n\\n\\n\\n\\n\\n\\n\\n\\nI. Gezeichnetes Kapital\\n\\n\\n38.961,00\\n38.961,00\\n\\n\\nII. Kapitalrücklage\\n\\n\\n5.156.459,00\\n5.156.459,00\\n\\n\\nIII. Verlustvortrag\\n\\n\\n664.181,41\\n154.457,86\\n\\n\\nIV. Jahresfehlbetrag\\n\\n\\n1.772.330,99\\n509.723,55\\n\\n\\nSumme Eigenkapital\\n\\n\\n2.758.907,60\\n4.531.238,59\\n\\n\\nB. Rückstellungen\\n\\n\\n564.924,70\\n224.166,39\\n\\n\\nC. Verbindlichkeiten\\n\\n\\n1.098.741,28\\n1.791.105,08\\n\\n\\n- davon mit einer Restlaufzeit bis\\n                     zu einem Jahr EUR 598.741,28\\n                     (EUR 1.289.301,43)\\n\\n\\n\\n\\n\\n\\n\\n\\n- davon mit einer Restlaufzeit von\\n                     mehr als einem Jahr\\n                     EUR 500.000,00\\n                     (EUR 501.803,65)\\n                     \\n\\n\\n\\n\\n\\n\\n\\n\\nD. Rechnungsabgrenzungsposten\\n\\n\\n234.776,46\\n0,00\\n\\n\\n\\n\\n\\n\\n4.657.350,04\\n6.546.510,06\\n\\n\\n\\nANHANG zum 31.12.2021\\n\\nAllgemeine Angaben zum Jahresabschluss\\n               \\n\\n\\nAngaben zur Identifikation der Gesellschaft laut Registergericht\\n               \\n\\n\\n\\n\\n\\n\\n\\n\\nFirmenname laut Registergericht: \\nMyra Security GmbH\\n\\n\\nFirmensitz laut Registergericht:\\nMünchen\\n\\n\\nRegistereintrag:\\nHandelsregister\\n\\n\\nRegistergericht: \\nMünchen\\n\\n\\nRegister-Nr.:\\nHRB 202428\\n\\n\\n\\nDer vorliegende Jahresabschluss wurde gemäß §§ 242ff. und 264ff. HGB sowie nach den\\n            einschlägigen\\n            Vorschriften des GmbHG aufgestellt. Es gelten die Vorschriften für kleine Kapitalgesellschaften.\\n            \\nDie Erleichterungsvorschriften für die Aufstellung des Jahresabschlusses gemäß §§\\n            274a und 288 Abs. 1\\n            HGB werden in Anspruch genommen.\\n            \\nDie Gewinn- und Verlustrechnung ist nach dem Gesamtkostenverfahren gegliedert.\\n            \\n\\nAngaben zu Bilanzierungs- und Bewertungsmethoden\\n               \\n\\n\\nBilanzierungs- und Bewertungsgrundsätze\\n               \\n\\nFür die Aufstellung des Jahresabschlusses waren unverändert die nachfolgenden Bilanzierungs-\\n            und\\n            Bewertungsmethoden maßgebend.\\n            \\nDer Jahresabschluss ist unter der Annahme der Fortführung der Unternehmenstätigkeit\\n            aufgestellt. Die\\n            Gesellschaft hat im abgelaufenen Geschäftsjahr einen mit Zahlungsmittelabflüssen verbundenen\\n            Jahresfehlbetrag in Höhe von EUR 1,8 Mio. erwirtschaftet. Die gesetzlichen Vertreter\\n            rechnen im\\n            Prognosezeitraum mit verbesserten, jedoch weiterhin negativen Jahresergebnissen. Dementsprechend\\n            weist\\n            die Liquiditätsplanung für die Folgemonate weiterhin negative operative Cashflows\\n            aus. Diese sollen durch\\n            die zum Aufstellungszeitpunkt bestehende Liquidität von EUR 2.479.631,61 abgedeckt\\n            werden und sind laut\\n            Liquiditätsplanung ausreichend, um die Zahlungsfähigkeit der Gesellschaft im Prognosezeitraum\\n            aufrechtzuerhalten.\\n            \\nDa die Planung mit Unsicherheiten behaftet ist, ist die Aufrechterhaltung der Zahlungsfähigkeit\\n            im\\n            Prognosezeitraum und damit die Fortführung der Unternehmenstätigkeit davon abhängig,\\n            dass die der\\n            Liquiditätsplanung zugrunde liegenden Prämissen, insbesondere zur Entwicklung der\\n            Umsatzerlöse sowie\\n            der operativen Kosten, eintreten beziehungsweise nicht wesentlich unterschritten werden.\\n            Werden diese\\n            Prämissen nicht erreicht oder wesentlich unterschritten, kann sich ein weiterer Finanzierungsbedarf\\n            ergeben,\\n            der eine zusätzliche Finanzierung durch die Gesellschafter oder eine anderweitige\\n            externe Finanzierung\\n            erforderlich machen würde. Die Gesellschaft wird zur Fortführung ihrer Unternehmenstätigkeit\\n            voraussichtlich nach Ablauf des Prognosezeitraums, bis zur Erreichung nachhaltiger\\n            positiver operativer\\n            Cashflows, von einer weiteren externen Zuführung liquider Mittel abhängig sein.\\n            Erworbene immaterielle Anlagewerte wurden zu Anschaffungskosten angesetzt und, sofern\\n            sie der\\n            Abnutzung unterlagen, um planmäßige Abschreibungen vermindert.\\n            \\nDas Sachanlagevermögen wurde zu Anschaffungs- bzw. Herstellungskosten angesetzt und,\\n            soweit\\n            abnutzbar, um planmäßige Abschreibungen vermindert.\\n            \\nDie planmäßigen Abschreibungen wurden nach der voraussichtlichen Nutzungsdauer (zwischen\\n            3 bis 13\\n            Jahren) der Vermögensgegenstände linear vorgenommen.\\n            \\nDie Forderungen und sonstigen Vermögensgegenstände werden mit dem Nennwert oder dem\\n            niedrigeren\\n            beizulegenden Wert am Bilanzstichtag angesetzt. Erkennbare Risiken wird durch die\\n            Berücksichtigung von\\n            Einzelwertberichtigungen Rechnung getragen.\\n            \\nKassenbestand und Guthaben bei Kreditinstituten sind zum Nominalwert angesetzt.\\n            \\nAls Rechnungsabgrenzungsposten sind auf der Aktivseite Ausgaben vor dem Abschlussstichtag\\n            ausgewiesen, soweit sie Aufwendungen für eine bestimmte Zeit nach diesem Tag darstellen.\\n            Auf der\\n            Passivseite sind Einnahmen vor dem Abschlussstichtag ausgewiesen, soweit sie Erträge\\n            für eine bestimmte\\n            Zeit nach dem Abschlussstichtag darstellen.\\n            \\nDie sonstigen Rückstellungen wurden für alle weiteren ungewissen Verbindlichkeiten\\n            gebildet. Dabei wurden\\n            alle erkennbaren Risiken berücksichtigt. Sie wurden zum Erfüllungsbetrag angesetzt.\\n            \\nVerbindlichkeiten wurden zum Erfüllungsbetrag angesetzt.\\n            \\n\\nAngaben zur Bilanz\\n               \\n\\nDie Forderungen und sonstigen Vermögensgegenstände haben sämtlich eine Restlaufzeit\\n            von bis zu einem\\n            Jahr.\\n            \\nEs bestehen keine Verbindlichkeiten mit einer Restlaufzeit von mehr als 5 Jahren (Vorjahr:\\n            EUR 0,00).\\n            \\nVon den Verbindlichkeiten sind EUR 0,00 gesichert durch die Abtretung von Forderungen\\n            in Form einer\\n            Globalzession (Vorjahr: EUR 641.440,50).\\n            \\n\\nHaftungsverhältnisse aus nicht bilanzierten sonstigen finanziellen Verpflichtungen\\n               \\n\\nNeben den in der Bilanz ausgewiesenen Verbindlichkeiten besteht in Höhe von EUR 44.000,00\\n            sonstige\\n            finanzielle Verpflichtungen aus Avale für die Büroräume.\\n            \\nIm Einzelnen beinhalten diese Verpflichtungen folgende Sachverhalte:\\n            \\nAvalkredit für Büroräume im Mietobjekt Landsberger Str. 187, 80687 München EUR 44.000,00\\n            \\nGegenwärtig sind keine Hinweise für eine Inanspruchnahme der Gesellschaft aus den\\n            eingegangenen\\n            Haftungsverhältnissen ersichtlich.\\n            \\n\\nSonstige Angaben\\n               \\n\\n\\nVirtuelles Mitarbeiterprogramm\\n               \\n\\nDie Gesellschaft hat ein virtuelles Mitarbeiterbeteiligungsprogramm aufgelegt. Zum\\n            31. Dezember 2021 sind\\n            767 virtuelle Anteile an Mitarbeiter zugeteilt worden. Über das Programm partizipieren\\n            die entsprechenden\\n            Mitarbeiter, wenn bestimmte Bedingungen eintreten. Die Bedingungen, die im Rahmen\\n            des\\n            Mitarbeiterbeteiligungsprogramms zu einer Zahlungsverpflichtung für die Gesellschaft\\n            führen, waren zum\\n            Bilanzstichtag nicht mit hinreichender Wahrscheinlichkeit erfüllt. Es werden entsprechend\\n            zum Bilanzstichtag\\n            keine Rückstellungen gebildet. Die Eintrittswahrscheinlichkeit der Bedingungen bewertet\\n            die\\n            Geschäftsführung für die Jahre 2022 bis 2024 als gering. Nichtsdestotrotz ist die\\n            Gesellschaft in Zukunft aus\\n            dem Beteiligungsprogramm belastet, wenn und soweit ein entsprechendes Ereignis eintritt.\\n            \\n\\nSonstige finanzielle Verpflichtungen\\n               \\n\\nAm Bilanzstichtag bestehen sonstige finanzielle Verpflichtungen aus bestehenden Mietverträgen\\n            für\\n            Büroräume sowie für Druckergeräte in Höhe von EUR 1.173.417,15. Die Verträge haben\\n            eine Laufzeit bis\\n            2024.\\n            \\n\\nDurchschnittliche Zahl der während des Geschäftsjahrs beschäftigten Arbeitnehmer\\n               \\n\\nDie durchschnittliche Zahl der während des Geschäftsjahres im Unternehmen beschäftigten\\n            Arbeitnehmer\\n            betrug 51.\\n            \\n\\xa0\\n\\nMünchen, den 25.05.2022\\ngez. Sascha SchumannGeschäftsführer\\ngez. Paul KaffsackGeschäftsführer\\n\\nDie Feststellung des Jahresabschlusses 2021 erfolgte laut Protokoll über die Beschlussfassung\\n            vom\\n            19.05.2022.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'raw_report': '<div class=\"publication_container\">\\n <div class=\"content-wrapper table-scroll\">\\n  <div class=\"scroll-wrapper\">\\n   <div class=\"publicationcontent scrollbar-visible\" id=\"preview_data\">\\n    <div class=\"forcemargin\">\\n    </div>\\n    <div>\\n     <table id=\"begin_pub\">\\n      <tr>\\n       <td>\\n       </td>\\n      </tr>\\n      <tr>\\n       <td>\\n        <h3 class=\"z_titel\">\\n         Myra Security GmbH\\n        </h3>\\n        <h4 class=\"z_titel\">\\n         München\\n        </h4>\\n        <h3 class=\"l_titel\">\\n         Jahresabschluss zum Geschäftsjahr vom 01.01.2021 bis zum 31.12.2021\\n        </h3>\\n        <h3 class=\"b_teil\" id=\"jp_BILANZ_zum_31._Dezember_2021\">\\n         BILANZ zum 31. Dezember 2021\\n        </h3>\\n        <p>\\n         <b>\\n          AKTIVA\\n         </b>\\n        </p>\\n        <table border=\"0\" cellpadding=\"2\" cellspacing=\"0\" class=\"std_table\" width=\"900\">\\n         <colgroup>\\n          <col/>\\n          <col width=\"15%\"/>\\n          <col width=\"15%\"/>\\n          <col width=\"15%\"/>\\n         </colgroup>\\n         <thead>\\n          <tr bgcolor=\"#cdcdce\">\\n           <td style=\"text-align:left;\">\\n           </td>\\n           <td colspan=\"2\" style=\"text-align:center;\">\\n            Geschäftsjahr\\n           </td>\\n           <td style=\"text-align:right;\">\\n            Vorjahr\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#cdcdce\">\\n           <td style=\"text-align:left;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            EUR\\n           </td>\\n           <td style=\"text-align:right;\">\\n            EUR\\n           </td>\\n           <td style=\"text-align:right;\">\\n            EUR\\n           </td>\\n          </tr>\\n         </thead>\\n         <tbody>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            A. Anlagevermögen\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            I. Immaterielle Vermögensgegenstände\\n           </td>\\n           <td style=\"text-align:right;\">\\n            399.321,07\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            229.063,00\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            II. Sachanlagen\\n           </td>\\n           <td style=\"text-align:right;\">\\n            935.797,80\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            709.159,05\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            1.335.118,87\\n           </td>\\n           <td style=\"text-align:right;\">\\n            938.222,05\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            B. Umlaufvermögen\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            I. Vorräte\\n           </td>\\n           <td style=\"text-align:right;\">\\n            51.625,54\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            0,00\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            II. Forderungen und sonstige Vermögensgegenstände\\n           </td>\\n           <td style=\"text-align:right;\">\\n            886.016,79\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            557.204,85\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            III. Kassenbestand,\\n                     Bundesbankguthaben, Guthaben bei\\n                     Kreditinstituten und Schecks\\n           </td>\\n           <td style=\"text-align:right;\">\\n            2.285.848,38\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            4.994.256,82\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            3.223.490,71\\n           </td>\\n           <td style=\"text-align:right;\">\\n            5.551.461,67\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            C. Rechnungsabgrenzungsposten\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            98.740,46\\n           </td>\\n           <td style=\"text-align:right;\">\\n            56.826,34\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            4.657.350,04\\n           </td>\\n           <td style=\"text-align:right;\">\\n            6.546.510,06\\n           </td>\\n          </tr>\\n         </tbody>\\n        </table>\\n        <p>\\n         <b>\\n          PASSIVA\\n         </b>\\n        </p>\\n        <table border=\"0\" cellpadding=\"2\" cellspacing=\"0\" class=\"std_table\" width=\"900\">\\n         <colgroup>\\n          <col/>\\n          <col width=\"15%\"/>\\n          <col width=\"15%\"/>\\n          <col width=\"15%\"/>\\n         </colgroup>\\n         <thead>\\n          <tr bgcolor=\"#cdcdce\">\\n           <td style=\"text-align:left;\">\\n           </td>\\n           <td colspan=\"2\" style=\"text-align:center;\">\\n            Geschäftsjahr\\n           </td>\\n           <td style=\"text-align:right;\">\\n            Vorjahr\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#cdcdce\">\\n           <td style=\"text-align:left;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            EUR\\n           </td>\\n           <td style=\"text-align:right;\">\\n            EUR\\n           </td>\\n           <td style=\"text-align:right;\">\\n            EUR\\n           </td>\\n          </tr>\\n         </thead>\\n         <tbody>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            A. Eigenkapital\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            I. Gezeichnetes Kapital\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            38.961,00\\n           </td>\\n           <td style=\"text-align:right;\">\\n            38.961,00\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            II. Kapitalrücklage\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            5.156.459,00\\n           </td>\\n           <td style=\"text-align:right;\">\\n            5.156.459,00\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            III. Verlustvortrag\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            664.181,41\\n           </td>\\n           <td style=\"text-align:right;\">\\n            154.457,86\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            IV. Jahresfehlbetrag\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            1.772.330,99\\n           </td>\\n           <td style=\"text-align:right;\">\\n            509.723,55\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            Summe Eigenkapital\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            2.758.907,60\\n           </td>\\n           <td style=\"text-align:right;\">\\n            4.531.238,59\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            B. Rückstellungen\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            564.924,70\\n           </td>\\n           <td style=\"text-align:right;\">\\n            224.166,39\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            C. Verbindlichkeiten\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            1.098.741,28\\n           </td>\\n           <td style=\"text-align:right;\">\\n            1.791.105,08\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            - davon mit einer Restlaufzeit bis\\n                     zu einem Jahr EUR 598.741,28\\n                     (EUR 1.289.301,43)\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            - davon mit einer Restlaufzeit von\\n                     mehr als einem Jahr\\n                     EUR 500.000,00\\n                     (EUR 501.803,65)\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            D. Rechnungsabgrenzungsposten\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            234.776,46\\n           </td>\\n           <td style=\"text-align:right;\">\\n            0,00\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n           </td>\\n           <td style=\"text-align:right;\">\\n            4.657.350,04\\n           </td>\\n           <td style=\"text-align:right;\">\\n            6.546.510,06\\n           </td>\\n          </tr>\\n         </tbody>\\n        </table>\\n        <h3 class=\"b_teil\" id=\"jp_ANHANG_zum_31.12.2021\">\\n         ANHANG zum 31.12.2021\\n        </h3>\\n        <p>\\n         <b>\\n          Allgemeine Angaben zum Jahresabschluss\\n         </b>\\n        </p>\\n        <p>\\n         <b>\\n          Angaben zur Identifikation der Gesellschaft laut Registergericht\\n         </b>\\n        </p>\\n        <table border=\"0\" cellpadding=\"2\" cellspacing=\"0\" class=\"std_table\" width=\"900\">\\n         <colgroup>\\n          <col/>\\n          <col/>\\n         </colgroup>\\n         <tbody>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            Firmenname laut Registergericht:\\n           </td>\\n           <td style=\"text-align:left;\">\\n            Myra Security GmbH\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            Firmensitz laut Registergericht:\\n           </td>\\n           <td style=\"text-align:left;\">\\n            München\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            Registereintrag:\\n           </td>\\n           <td style=\"text-align:left;\">\\n            Handelsregister\\n           </td>\\n          </tr>\\n          <tr bgcolor=\"#f0f0f0\">\\n           <td style=\"text-align:left;\">\\n            Registergericht:\\n           </td>\\n           <td style=\"text-align:left;\">\\n            München\\n           </td>\\n          </tr>\\n          <tr>\\n           <td style=\"text-align:left;\">\\n            Register-Nr.:\\n           </td>\\n           <td style=\"text-align:left;\">\\n            HRB 202428\\n           </td>\\n          </tr>\\n         </tbody>\\n        </table>\\n        <p>\\n         Der vorliegende Jahresabschluss wurde gemäß §§ 242ff. und 264ff. HGB sowie nach den\\n            einschlägigen\\n            Vorschriften des GmbHG aufgestellt. Es gelten die Vorschriften für kleine Kapitalgesellschaften.\\n        </p>\\n        <p>\\n         Die Erleichterungsvorschriften für die Aufstellung des Jahresabschlusses gemäß §§\\n            274a und 288 Abs. 1\\n            HGB werden in Anspruch genommen.\\n        </p>\\n        <p>\\n         Die Gewinn- und Verlustrechnung ist nach dem Gesamtkostenverfahren gegliedert.\\n        </p>\\n        <p>\\n         <b>\\n          Angaben zu Bilanzierungs- und Bewertungsmethoden\\n         </b>\\n        </p>\\n        <p>\\n         <b>\\n          Bilanzierungs- und Bewertungsgrundsätze\\n         </b>\\n        </p>\\n        <p>\\n         Für die Aufstellung des Jahresabschlusses waren unverändert die nachfolgenden Bilanzierungs-\\n            und\\n            Bewertungsmethoden maßgebend.\\n        </p>\\n        <p>\\n         Der Jahresabschluss ist unter der Annahme der Fortführung der Unternehmenstätigkeit\\n            aufgestellt. Die\\n            Gesellschaft hat im abgelaufenen Geschäftsjahr einen mit Zahlungsmittelabflüssen verbundenen\\n            Jahresfehlbetrag in Höhe von EUR 1,8 Mio. erwirtschaftet. Die gesetzlichen Vertreter\\n            rechnen im\\n            Prognosezeitraum mit verbesserten, jedoch weiterhin negativen Jahresergebnissen. Dementsprechend\\n            weist\\n            die Liquiditätsplanung für die Folgemonate weiterhin negative operative Cashflows\\n            aus. Diese sollen durch\\n            die zum Aufstellungszeitpunkt bestehende Liquidität von EUR 2.479.631,61 abgedeckt\\n            werden und sind laut\\n            Liquiditätsplanung ausreichend, um die Zahlungsfähigkeit der Gesellschaft im Prognosezeitraum\\n            aufrechtzuerhalten.\\n        </p>\\n        <p>\\n         Da die Planung mit Unsicherheiten behaftet ist, ist die Aufrechterhaltung der Zahlungsfähigkeit\\n            im\\n            Prognosezeitraum und damit die Fortführung der Unternehmenstätigkeit davon abhängig,\\n            dass die der\\n            Liquiditätsplanung zugrunde liegenden Prämissen, insbesondere zur Entwicklung der\\n            Umsatzerlöse sowie\\n            der operativen Kosten, eintreten beziehungsweise nicht wesentlich unterschritten werden.\\n            Werden diese\\n            Prämissen nicht erreicht oder wesentlich unterschritten, kann sich ein weiterer Finanzierungsbedarf\\n            ergeben,\\n            der eine zusätzliche Finanzierung durch die Gesellschafter oder eine anderweitige\\n            externe Finanzierung\\n            erforderlich machen würde. Die Gesellschaft wird zur Fortführung ihrer Unternehmenstätigkeit\\n            voraussichtlich nach Ablauf des Prognosezeitraums, bis zur Erreichung nachhaltiger\\n            positiver operativer\\n            Cashflows, von einer weiteren externen Zuführung liquider Mittel abhängig sein.\\n            Erworbene immaterielle Anlagewerte wurden zu Anschaffungskosten angesetzt und, sofern\\n            sie der\\n            Abnutzung unterlagen, um planmäßige Abschreibungen vermindert.\\n        </p>\\n        <p>\\n         Das Sachanlagevermögen wurde zu Anschaffungs- bzw. Herstellungskosten angesetzt und,\\n            soweit\\n            abnutzbar, um planmäßige Abschreibungen vermindert.\\n        </p>\\n        <p>\\n         Die planmäßigen Abschreibungen wurden nach der voraussichtlichen Nutzungsdauer (zwischen\\n            3 bis 13\\n            Jahren) der Vermögensgegenstände linear vorgenommen.\\n        </p>\\n        <p>\\n         Die Forderungen und sonstigen Vermögensgegenstände werden mit dem Nennwert oder dem\\n            niedrigeren\\n            beizulegenden Wert am Bilanzstichtag angesetzt. Erkennbare Risiken wird durch die\\n            Berücksichtigung von\\n            Einzelwertberichtigungen Rechnung getragen.\\n        </p>\\n        <p>\\n         Kassenbestand und Guthaben bei Kreditinstituten sind zum Nominalwert angesetzt.\\n        </p>\\n        <p>\\n         Als Rechnungsabgrenzungsposten sind auf der Aktivseite Ausgaben vor dem Abschlussstichtag\\n            ausgewiesen, soweit sie Aufwendungen für eine bestimmte Zeit nach diesem Tag darstellen.\\n            Auf der\\n            Passivseite sind Einnahmen vor dem Abschlussstichtag ausgewiesen, soweit sie Erträge\\n            für eine bestimmte\\n            Zeit nach dem Abschlussstichtag darstellen.\\n        </p>\\n        <p>\\n         Die sonstigen Rückstellungen wurden für alle weiteren ungewissen Verbindlichkeiten\\n            gebildet. Dabei wurden\\n            alle erkennbaren Risiken berücksichtigt. Sie wurden zum Erfüllungsbetrag angesetzt.\\n        </p>\\n        <p>\\n         Verbindlichkeiten wurden zum Erfüllungsbetrag angesetzt.\\n        </p>\\n        <p>\\n         <b>\\n          Angaben zur Bilanz\\n         </b>\\n        </p>\\n        <p>\\n         Die Forderungen und sonstigen Vermögensgegenstände haben sämtlich eine Restlaufzeit\\n            von bis zu einem\\n            Jahr.\\n        </p>\\n        <p>\\n         Es bestehen keine Verbindlichkeiten mit einer Restlaufzeit von mehr als 5 Jahren (Vorjahr:\\n            EUR 0,00).\\n        </p>\\n        <p>\\n         Von den Verbindlichkeiten sind EUR 0,00 gesichert durch die Abtretung von Forderungen\\n            in Form einer\\n            Globalzession (Vorjahr: EUR 641.440,50).\\n        </p>\\n        <p>\\n         <b>\\n          Haftungsverhältnisse aus nicht bilanzierten sonstigen finanziellen Verpflichtungen\\n         </b>\\n        </p>\\n        <p>\\n         Neben den in der Bilanz ausgewiesenen Verbindlichkeiten besteht in Höhe von EUR 44.000,00\\n            sonstige\\n            finanzielle Verpflichtungen aus Avale für die Büroräume.\\n        </p>\\n        <p>\\n         Im Einzelnen beinhalten diese Verpflichtungen folgende Sachverhalte:\\n        </p>\\n        <p>\\n         Avalkredit für Büroräume im Mietobjekt Landsberger Str. 187, 80687 München EUR 44.000,00\\n        </p>\\n        <p>\\n         Gegenwärtig sind keine Hinweise für eine Inanspruchnahme der Gesellschaft aus den\\n            eingegangenen\\n            Haftungsverhältnissen ersichtlich.\\n        </p>\\n        <p>\\n         <b>\\n          Sonstige Angaben\\n         </b>\\n        </p>\\n        <p>\\n         <b>\\n          Virtuelles Mitarbeiterprogramm\\n         </b>\\n        </p>\\n        <p>\\n         Die Gesellschaft hat ein virtuelles Mitarbeiterbeteiligungsprogramm aufgelegt. Zum\\n            31. Dezember 2021 sind\\n            767 virtuelle Anteile an Mitarbeiter zugeteilt worden. Über das Programm partizipieren\\n            die entsprechenden\\n            Mitarbeiter, wenn bestimmte Bedingungen eintreten. Die Bedingungen, die im Rahmen\\n            des\\n            Mitarbeiterbeteiligungsprogramms zu einer Zahlungsverpflichtung für die Gesellschaft\\n            führen, waren zum\\n            Bilanzstichtag nicht mit hinreichender Wahrscheinlichkeit erfüllt. Es werden entsprechend\\n            zum Bilanzstichtag\\n            keine Rückstellungen gebildet. Die Eintrittswahrscheinlichkeit der Bedingungen bewertet\\n            die\\n            Geschäftsführung für die Jahre 2022 bis 2024 als gering. Nichtsdestotrotz ist die\\n            Gesellschaft in Zukunft aus\\n            dem Beteiligungsprogramm belastet, wenn und soweit ein entsprechendes Ereignis eintritt.\\n        </p>\\n        <p>\\n         <b>\\n          Sonstige finanzielle Verpflichtungen\\n         </b>\\n        </p>\\n        <p>\\n         Am Bilanzstichtag bestehen sonstige finanzielle Verpflichtungen aus bestehenden Mietverträgen\\n            für\\n            Büroräume sowie für Druckergeräte in Höhe von EUR 1.173.417,15. Die Verträge haben\\n            eine Laufzeit bis\\n            2024.\\n        </p>\\n        <p>\\n         <b>\\n          Durchschnittliche Zahl der während des Geschäftsjahrs beschäftigten Arbeitnehmer\\n         </b>\\n        </p>\\n        <p>\\n         Die durchschnittliche Zahl der während des Geschäftsjahres im Unternehmen beschäftigten\\n            Arbeitnehmer\\n            betrug 51.\\n        </p>\\n        <p>\\n        </p>\\n        <div style=\"font-family: Verdana; Arial Unicode MS; font-size: 10pt; text-align: center; font-weight: bold;\">\\n         <p style=\"text-align: left;\">\\n          München, den 25.05.2022\\n         </p>\\n         <p style=\"font-family: Verdana; Arial Unicode MS; font-size: 10pt; text-align: center\">\\n          <i>\\n           gez. Sascha Schumann\\n           <br/>\\n           Geschäftsführer\\n          </i>\\n         </p>\\n         <p style=\"font-family: Verdana; Arial Unicode MS; font-size: 10pt; text-align: center\">\\n          <i>\\n           gez. Paul Kaffsack\\n           <br/>\\n           Geschäftsführer\\n          </i>\\n         </p>\\n        </div>\\n        <p>\\n         Die Feststellung des Jahresabschlusses 2021 erfolgte laut Protokoll über die Beschlussfassung\\n            vom\\n            19.05.2022.\\n        </p>\\n       </td>\\n      </tr>\\n      <tr>\\n       <td>\\n       </td>\\n      </tr>\\n     </table>\\n    </div>\\n   </div>\\n  </div>\\n </div>\\n</div>\\n'}"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['14da79142f91a0e7bb80747555befe6d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "6454816a-37ac-4086-9487-b952021c48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = run_selenium('https://www.link11.com/en/solutions/web-protection/web-ddos-protection/')\n",
    "clean_content = clean_body_content(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "09011043-1c70-427e-bc87-050f3306fea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Web DDoS Protection - Link11\\nSolutions\\nNetwork Security\\nInfrastructure DDoS Protection\\nCloud Insights\\nWeb Protection\\nWeb DDoS Protection\\nBot Management\\nZero Touch WAF\\nAPI Protection\\nWeb Performance\\nSecure CDN\\nSecure DNS\\nIndustries\\nHosting\\nLogistic\\nBanking & Finance\\nE-Commerce\\nUtilities\\nOnline Gaming\\nMedia\\nPublic Sector\\nBenchmark\\nCustomers\\nResources\\nBlog\\nGlossary\\nPress\\nDownloads\\nVideos & Podcasts\\nEvents\\nAbout Link11\\nCompany\\nPatented DDoS Protection\\nPartner\\nCareer\\nCarbon Neutrality\\nContact\\nen\\nde\\nLogin\\nUnder attack?\\nContact\\nWeb DDoS Protection\\nCloud-based DDoS protection for web applications\\nAutomation ensures 24/7 protection\\nZero time to mitigate for known,\\n<\\n10 seconds for new vectors\\n4.3/5 Google Reviews\\nBook a demo\\nLearn more\\nPrecise detection\\nNo need to worry about complex attacks anymore: The system detects traffic anomalies and protects you from emerging threats in real time.\\nFull automation\\nNo work for you: manual operation is not required, and the system works fully automatically and without compromise.\\nDetailed customization\\nWhitelisting, blacklisting, enhanced logs, traffic control, and more – helpful features help you make everyday life as easy as possible.\\nBook a demo\\n* Mandatory Fields\\nYour name\\nHow can we reach you?\\nAbout your organization:\\nCountry*\\nGermany\\nAustria\\nSwitzerland\\nUnited Kingdom\\nIreland\\nUnited States\\nCanada\\nAndorra\\nUnited Arab Emirates\\nAfghanistan\\nAntigua and Barbuda\\nAnguilla\\nAlbania\\nArmenia\\nAngola\\nAntarctica\\nArgentina\\nAustralia\\nAruba\\nAland Islands\\nAzerbaijan\\nBosnia and Herzegovina\\nBarbados\\nBangladesh\\nBelgium\\nBurkina Faso\\nBulgaria\\nBahrain\\nBurundi\\nBenin\\nSaint Barthélemy\\nBermuda\\nBrunei Darussalam\\nBolivia, Plurinational State of\\nBonaire, Sint Eustatius and Saba\\nBrazil\\nBahamas\\nBhutan\\nBouvet Island\\nBotswana\\nBelarus\\nBelize\\nCocos (Keeling) Islands\\nCongo, the Democratic Republic of the\\nCentral African Republic\\nCongo\\nCote d'Ivoire\\nCook Islands\\nChile\\nCameroon\\nChina\\nColombia\\nCosta Rica\\nCuba\\nCape Verde\\nCuraçao\\nChristmas Island\\nCyprus\\nCzech Republic\\nDjibouti\\nDenmark\\nDominica\\nDominican Republic\\nAlgeria\\nEcuador\\nEstonia\\nEgypt\\nWestern Sahara\\nEritrea\\nSpain\\nEthiopia\\nFinland\\nFiji\\nFalkland Islands (Malvinas)\\nFaroe Islands\\nFrance\\nGabon\\nGrenada\\nGeorgia\\nFrench Guiana\\nGuernsey\\nGhana\\nGibraltar\\nGreenland\\nGambia\\nGuinea\\nGuadeloupe\\nEquatorial Guinea\\nGreece\\nSouth Georgia and the South Sandwich Islands\\nGuatemala\\nGuinea-Bissau\\nGuyana\\nHong Kong\\nHeard Island and McDonald Islands\\nHonduras\\nCroatia\\nHaiti\\nHungary\\nIndonesia\\nIsrael\\nIsle of Man\\nIndia\\nBritish Indian Ocean Territory\\nIraq\\nIran\\nIceland\\nItaly\\nJersey\\nJamaica\\nJordan\\nJapan\\nKenya\\nKyrgyzstan\\nCambodia\\nKiribati\\nComoros\\nSaint Kitts and Nevis\\nKorea, Democratic People's Republic of\\nSouth Korea\\nKuwait\\nCayman Islands\\nKazakhstan\\nLao People's Democratic Republic\\nLebanon\\nSaint Lucia\\nLiechtenstein\\nSri Lanka\\nLiberia\\nLesotho\\nLithuania\\nLuxembourg\\nLatvia\\nLibyan Arab Jamahiriya\\nMorocco\\nMonaco\\nMoldova, Republic of\\nMontenegro\\nSaint Martin (French part)\\nMadagascar\\nMacedonia, the former Yugoslav Republic of\\nMali\\nMyanmar\\nMongolia\\nMacao\\nMartinique\\nMauritania\\nMontserrat\\nMalta\\nMauritius\\nMaldives\\nMalawi\\nMexico\\nMalaysia\\nMozambique\\nNamibia\\nNew Caledonia\\nNiger\\nNorfolk Island\\nNigeria\\nNicaragua\\nNetherlands\\nNorway\\nNepal\\nNauru\\nNiue\\nNew Zealand\\nOman\\nPanama\\nPeru\\nFrench Polynesia\\nPapua New Guinea\\nPhilippines\\nPakistan\\nPoland\\nSaint Pierre and Miquelon\\nPitcairn\\nPalestinian Territory, Occupied\\nPortugal\\nParaguay\\nQatar\\nReunion\\nRomania\\nSerbia\\nRussian Federation\\nRwanda\\nSaudi Arabia\\nSolomon Islands\\nSeychelles\\nSudan\\nSweden\\nSingapore\\nSaint Helena, Ascension and Tristan da Cunha\\nSlovenia\\nSvalbard and Jan Mayen\\nSlovakia\\nSierra Leone\\nSan Marino\\nSenegal\\nSomalia\\nSuriname\\nSouth Sudan\\nSao Tome and Principe\\nEl Salvador\\nSint Maarten (Dutch part)\\nSyria\\nSwaziland\\nTurks and Caicos Islands\\nChad\\nFrench Southern Territories\\nTogo\\nThailand\\nTajikistan\\nTokelau\\nTimor-Leste\\nTurkmenistan\\nTunisia\\nTonga\\nTurkey\\nTrinidad and Tobago\\nTuvalu\\nTaiwan\\nTanzania, United Republic of\\nUkraine\\nUganda\\nUruguay\\nUzbekistan\\nHoly See (Vatican City State)\\nSaint Vincent and the Grenadines\\nVenezuela, Bolivarian Republic of\\nVirgin Islands, British\\nVietnam\\nVanuatu\\nWallis and Futuna\\nSamoa\\nYemen\\nMayotte\\nSouth Africa\\nZambia\\nZimbabwe\\nWould you like to give us additional information to speed up the consultation process?\\nYes, I would like to answer further questions.\\nHow many websites are in operation?\\nHow many backend IPs are in use?\\nHow much traffic is measured in TB?\\nHow many requests are there per month?\\nWhat type of certificate do you use?\\nHow big is your network? (e.g. /24)\\nHow can we reach you?\\nMessage\\nLoading...\\nBy sending this form I confirm that I have read and understood the\\nprivacy policy\\n.*\\nAll possibilities\\nwith one solution\\nDDoS attacks continue to increase and still cause great damage today. In addition to the frequency, the complexity and duration of an attack cause major problems for the defense. Therefore, the deployed solution should intervene precisely and quickly to best protect you from threats and high costs due to downtime.\\nGuaranteed protection\\nWith Web DDoS Protection, you’ll be protected by a system that, thanks to artificial intelligence, effectively stops attacks on layers 3, 4, and 7. You’ll also benefit from numerous algorithms and heuristics that monitor and evaluate events at the application layer. With this combination, you can count on guaranteed mitigation of the attack within the shortest possible time.\\nOur system analyzes the typical traffic patterns of your web applications and identifies any anomalies that deviate from the “normal” or appear suspicious. This proactive approach ensures your protection not only against known threats but also against new and unknown ones.\\nMore focus on your core business\\nOur DDoS Protection makes your job as easy as possible. That’s why the security solution’s detection and mitigation tools work entirely without manual intervention. Attacks are detected and mitigated in real-time without you having to interact with the protection. This ensures that you and your applications are protected around the clock, at any time of day.\\nThis automated security saves time and money and makes it even easier for you to focus on what matters most. Another advantage is that you don’t need to contact us in the event of an attack. Our system already knows and has taken all the necessary steps to mitigate the threat. Your everyday business can, therefore, continue as if nothing had happened.\\nExactly the way you need it\\nIndividual settings help you tailor the system ideally to your needs – our Web DDoS Protection offers just that. Provide the solution with your own certificate or use one from us, set up TCP port forwarding, or modify the geoblocking function. The choice is entirely yours.\\nThe choices mean you don’t have to work with a rigid system that dictates what you can and can’t do. Take advantage of the flexibility and tailor your settings to the exact situation.\\nFeatures that make a difference\\nAccess Logging & Monitoring\\nOur comprehensive access logs and interactive dashboard visualizations give unparalleled transparency and detailed insights into your network’s traffic patterns. Our access logs meticulously record every connection, offering a granular view of requester activity during regular operations and amidst attack scenarios.\\nBut we don’t stop there – our user-friendly dashboards transform this data into actionable visual representations, making it effortless for you to monitor, analyze, and strategize your defense. Trust is built on transparency, and our access logs and dashboards empower you with the knowledge you need to stay informed and secure.\\nFlexible IP & Geoblocking\\nShape your defense strategy to perfection with our customizable IP, Geo, and ASN Blocking Algorithms, offering you the ultimate control over your security measures. These dynamic algorithms allow you to finely adjust your protection, ensuring a harmonious balance between robust security and reducing false positives to a minimum.\\nWhether you need to block specific IPs, regions, or ASNs (Autonomous System Numbers), our comprehensive suite of blocking options allows you to create a tailored security solution that perfectly aligns with your organization’s unique requirements thanks to Whitelisting.\\nCaptcha Defense for suspicious IPs\\nSafeguard your network against suspicious IPs with our robust Captcha verification system, effectively filtering out automated and bot traffic while allowing legitimate users to access your services seamlessly. What sets us apart is our commitment to privacy and data protection.\\nWith this added layer of security, you can confidently protect your network while respecting user privacy and legal requirements.\\nEnhanced Web Traffic Management\\nElevate your network’s performance with an array of advanced features, including Redirects, Origin Load Balancing, and Origin Timeouts. Redirects efficiently steer unencrypted HTTP traffic towards secure HTTPS connections at the edge, enhancing data security. Meanwhile, Origin Load Balancing distributes incoming traffic intelligently across multiple servers within your architecture, ensuring optimal resource utilization and minimal latency.\\nFinally, Origin Timeouts empowers you to set precise connection and read timeout values, preventing requests from exceeding defined limits and enhancing overall network efficiency. These versatile tools collectively optimize traffic distribution and help you achieve and maintain peak performance for your online services.\\nEasy to use, powerful in the result\\nEverything at a glance: The dashboard shows key metrics, threat data, attacks averted, and data on bandwidth saved or traffic consumed. The flexibility of the data display is particularly noteworthy, as historical data can also be displayed via a data picker.\\n1\\nReporting allows creating individual reports and scheduled reports, which can also be exported to PDF. In addition, there is function to send reports automatically at specified times.\\n2\\nUser management gives administrators a detailed overview. User rights and security information can thus be conveniently checked and assigned. Details, such as the time of the last password change, and the activation of the two-factor authentication procedure can be found here.\\n3\\nAlarming: The notification frequency can be set in the contact settings. This allows customization so that users can be selected by products or areas of responsibility, for example. This way, only selected people can receive notifications and communication becomes more efficient.\\n4\\nEnhanced password management: Alphanumeric passwords and a configurable password length ensure that access is and remains secure. You can also use the WebGUI to set the maximum lifespan of passwords, prevent the reuse of older passwords and set a predefined limit for failed login attempts.\\n5\\nWhy you can rely on\\nLink11\\nFast & precise protection\\nWe protect more than two million assets of well-known customers worldwide. Our technology has more than proven itself during that process.\\nEasy to set-up\\nOur solutions are easy to integrate into any set-up. It is important for us that the effort for you is reduced to an absolute minimum.\\n24/7 Customer service\\nFor us, being there for you at any time is important. That’s why we offer 24/7 customer service in English and German to assist you in all topics.\\nCertified & qualified\\nFully compliant according to the strict EU data privacy laws. Also, ISO 27001 certified and officially qualified for the CRITIS sector by the Federal Office for Information Security (BSI).\\nEasily expanded\\nAlthough Web DDoS Protection already provides a wide range of security, you can easily extend the protection standard with additional services.\\n“A failure of our systems would not only have consequences for us, but also for the many platform participants. At Link11, we are glad that they work reliably, quickly, openly and transparently. That reinforces trust in this service provider.”\\nStefan Feller\\nDepartment head IT infrastructure\\nPharma mall\\nImportant Certificates & Partnerships\\nWould you like to learn more?\\nPowerful Web Application Protection\\nTogether, we will create a customized solution for protecting your web applications. Our security experts will happily support you and advise you without obligation on the benefits of our Link11 solutions for your company.\\nBook a demo\\nBook a demo\\n* Mandatory Fields\\nYour name\\nHow can we reach you?\\nAbout your organization:\\nCountry*\\nGermany\\nAustria\\nSwitzerland\\nUnited Kingdom\\nIreland\\nUnited States\\nCanada\\nAndorra\\nUnited Arab Emirates\\nAfghanistan\\nAntigua and Barbuda\\nAnguilla\\nAlbania\\nArmenia\\nAngola\\nAntarctica\\nArgentina\\nAustralia\\nAruba\\nAland Islands\\nAzerbaijan\\nBosnia and Herzegovina\\nBarbados\\nBangladesh\\nBelgium\\nBurkina Faso\\nBulgaria\\nBahrain\\nBurundi\\nBenin\\nSaint Barthélemy\\nBermuda\\nBrunei Darussalam\\nBolivia, Plurinational State of\\nBonaire, Sint Eustatius and Saba\\nBrazil\\nBahamas\\nBhutan\\nBouvet Island\\nBotswana\\nBelarus\\nBelize\\nCocos (Keeling) Islands\\nCongo, the Democratic Republic of the\\nCentral African Republic\\nCongo\\nCote d'Ivoire\\nCook Islands\\nChile\\nCameroon\\nChina\\nColombia\\nCosta Rica\\nCuba\\nCape Verde\\nCuraçao\\nChristmas Island\\nCyprus\\nCzech Republic\\nDjibouti\\nDenmark\\nDominica\\nDominican Republic\\nAlgeria\\nEcuador\\nEstonia\\nEgypt\\nWestern Sahara\\nEritrea\\nSpain\\nEthiopia\\nFinland\\nFiji\\nFalkland Islands (Malvinas)\\nFaroe Islands\\nFrance\\nGabon\\nGrenada\\nGeorgia\\nFrench Guiana\\nGuernsey\\nGhana\\nGibraltar\\nGreenland\\nGambia\\nGuinea\\nGuadeloupe\\nEquatorial Guinea\\nGreece\\nSouth Georgia and the South Sandwich Islands\\nGuatemala\\nGuinea-Bissau\\nGuyana\\nHong Kong\\nHeard Island and McDonald Islands\\nHonduras\\nCroatia\\nHaiti\\nHungary\\nIndonesia\\nIsrael\\nIsle of Man\\nIndia\\nBritish Indian Ocean Territory\\nIraq\\nIran\\nIceland\\nItaly\\nJersey\\nJamaica\\nJordan\\nJapan\\nKenya\\nKyrgyzstan\\nCambodia\\nKiribati\\nComoros\\nSaint Kitts and Nevis\\nKorea, Democratic People's Republic of\\nSouth Korea\\nKuwait\\nCayman Islands\\nKazakhstan\\nLao People's Democratic Republic\\nLebanon\\nSaint Lucia\\nLiechtenstein\\nSri Lanka\\nLiberia\\nLesotho\\nLithuania\\nLuxembourg\\nLatvia\\nLibyan Arab Jamahiriya\\nMorocco\\nMonaco\\nMoldova, Republic of\\nMontenegro\\nSaint Martin (French part)\\nMadagascar\\nMacedonia, the former Yugoslav Republic of\\nMali\\nMyanmar\\nMongolia\\nMacao\\nMartinique\\nMauritania\\nMontserrat\\nMalta\\nMauritius\\nMaldives\\nMalawi\\nMexico\\nMalaysia\\nMozambique\\nNamibia\\nNew Caledonia\\nNiger\\nNorfolk Island\\nNigeria\\nNicaragua\\nNetherlands\\nNorway\\nNepal\\nNauru\\nNiue\\nNew Zealand\\nOman\\nPanama\\nPeru\\nFrench Polynesia\\nPapua New Guinea\\nPhilippines\\nPakistan\\nPoland\\nSaint Pierre and Miquelon\\nPitcairn\\nPalestinian Territory, Occupied\\nPortugal\\nParaguay\\nQatar\\nReunion\\nRomania\\nSerbia\\nRussian Federation\\nRwanda\\nSaudi Arabia\\nSolomon Islands\\nSeychelles\\nSudan\\nSweden\\nSingapore\\nSaint Helena, Ascension and Tristan da Cunha\\nSlovenia\\nSvalbard and Jan Mayen\\nSlovakia\\nSierra Leone\\nSan Marino\\nSenegal\\nSomalia\\nSuriname\\nSouth Sudan\\nSao Tome and Principe\\nEl Salvador\\nSint Maarten (Dutch part)\\nSyria\\nSwaziland\\nTurks and Caicos Islands\\nChad\\nFrench Southern Territories\\nTogo\\nThailand\\nTajikistan\\nTokelau\\nTimor-Leste\\nTurkmenistan\\nTunisia\\nTonga\\nTurkey\\nTrinidad and Tobago\\nTuvalu\\nTaiwan\\nTanzania, United Republic of\\nUkraine\\nUganda\\nUruguay\\nUzbekistan\\nHoly See (Vatican City State)\\nSaint Vincent and the Grenadines\\nVenezuela, Bolivarian Republic of\\nVirgin Islands, British\\nVietnam\\nVanuatu\\nWallis and Futuna\\nSamoa\\nYemen\\nMayotte\\nSouth Africa\\nZambia\\nZimbabwe\\nWould you like to give us additional information to speed up the consultation process?\\nYes, I would like to answer further questions.\\nHow many websites are in operation?\\nHow many backend IPs are in use?\\nHow much traffic is measured in TB?\\nHow many requests are there per month?\\nWhat type of certificate do you use?\\nHow big is your network? (e.g. /24)\\nHow can we reach you?\\nMessage\\nLoading...\\nBy sending this form I confirm that I have read and understood the\\nprivacy policy\\n.*\\nModal 2\\nDeutschland (Hauptsitz)\\nLindleystraße 12\\n60314 Frankfurt\\n+49 69\\n5800492677\\nKontakt\\nUK & International\\n+49 69 580 049 263 05\\nKontakt\\nNordamerika\\n+1 888 818 1344\\nKontakt\\nAbout Link11\\nPartner\\nCareer\\nBlog\\nSolutions\\nDownloads\\nPress\\nContact\\nImprint\\nPrivacy Policy\\nEULA\\nGeneric selectors\\nExact matches only\\nSearch in title\\nSearch in content\\nPost Type Selectors\\nX\\nFacebook\\nTwitter\\nLinkedIn\\nWhatsapp\""
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "74492cb3-3358-4aff-83b6-99cac0def0cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[229], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minitialize_dumbresearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[225], line 442\u001b[0m, in \u001b[0;36minitialize_dumbresearch\u001b[0;34m(players)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_dumbresearch\u001b[39m(players \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 442\u001b[0m     domains_selected \u001b[38;5;241m=\u001b[39m \u001b[43muser_selects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplayers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m     parse_description \u001b[38;5;241m=\u001b[39m user_task()\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m domains_selected, parse_description\n",
      "Cell \u001b[0;32mIn[225], line 427\u001b[0m, in \u001b[0;36muser_selects\u001b[0;34m(players)\u001b[0m\n\u001b[1;32m    424\u001b[0m     domains\u001b[38;5;241m.\u001b[39mextend(chosen_links)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     domains \u001b[38;5;241m=\u001b[39m \u001b[43mselect_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m domain_links \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m domain \u001b[38;5;129;01min\u001b[39;00m domains:\n",
      "Cell \u001b[0;32mIn[225], line 388\u001b[0m, in \u001b[0;36mselect_topics\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^(https?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/)?([\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw.-]+)+(:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)?(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw._-]*)*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/?$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(pattern, url)\n\u001b[0;32m--> 388\u001b[0m website_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mPlease paste the websites you\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md like to explore, separated by commas: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m website_input\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo websites entered. Please try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/ipykernel/kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/ipykernel/kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "initialize_dumbresearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09bb9f",
   "metadata": {},
   "source": [
    "# User pastes in links they want to look at. Programm takes links and asks \n",
    "* which part of the website the user wants to look at\n",
    "* for each of the subsection what they want the output to look like and a description\n",
    "* where they want the output to be saved\n",
    "\n",
    "# Program runs for each website, creates the tabular output for each of the websites subsections (e.g. product, locations)\n",
    "\n",
    "# Tabular output is saved to the specified location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c1de4",
   "metadata": {},
   "source": [
    "Appendix\n",
    "\n",
    "parse_description = ('These are the solutions offered by a german cyber security player (input is provided in German), please provide a table or tables that list the respective products / solutions with important information on the respective products e.g. their descriptions, features, target customers, KPIs')\n",
    "\n",
    "#Function to extract deeper-level headlines\n",
    "def extract_headlines_with_depth(links, depth):\n",
    "    headlines = set()\n",
    "    for link in links:\n",
    "        try:\n",
    "            html = run_selenium(domain)\n",
    "            time.sleep(10)\n",
    "\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            for a_tag in soup.find_all('a', href=True):\n",
    "                # Resolve relative URLs\n",
    "                full_url = urljoin(link, a_tag['href'])\n",
    "                parsed = urlparse(full_url)\n",
    "                path_parts = parsed.path.strip('/').split('/')\n",
    "                if len(path_parts) == depth:\n",
    "                    headlines.append(full_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error visiting {full_url}: {e}\")\n",
    "            parsed = urlparse(link)\n",
    "            path_parts = parsed.path.strip('/').split('/')\n",
    "            if len(path_parts) == depth:\n",
    "                headline = \"/\".join(path_parts[:depth])\n",
    "                headlines.add(headline)\n",
    "    return sorted(headlines)\n",
    "\n",
    "\n",
    "def extract_second_level_headlines(links):\n",
    "    # Parse URLs and extract paths\n",
    "    headlines = set()  # Use a set to avoid duplicates\n",
    "    for link in links:\n",
    "        parsed = urlparse(link)\n",
    "        path_parts = parsed.path.strip('/').split('/')\n",
    "\n",
    "        # Match second-level paths (e.g., /solutions/, /locations/)\n",
    "        headline = \"/\".join(path_parts[:1])\n",
    "        headlines.add(headline)\n",
    "\n",
    "    return sorted(headlines)\n",
    "    \n",
    "def get_select_data(headlines, domain,depth = 3):\n",
    "    domain = domain\n",
    "    subpage_links = set(get_all_links(domain))  # Assuming this fetches all sub-links from the domain\n",
    "    \n",
    "    # Dictionary to store concatenated text for each key\n",
    "    content_dict = defaultdict(str)\n",
    "\n",
    "    # Fetch and process links\n",
    "    urls = {}\n",
    "    for page in headlines:\n",
    "        # Match URLs for the current page\n",
    "        urls[page] = re_match_second_level_headlines(page, domain, subpage_links)\n",
    "        for url in urls[page]:  # Iterate over the URLs for the current page\n",
    "            if not is_valid_url(url):  # Validate URL before scraping\n",
    "                print(f\"Invalid URL skipped: {url}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Scraping URL: {url}\")\n",
    "            result = run_selenium(url)  # Scrape website content\n",
    "            body_content = extract_body_content(result)  # Extract body content\n",
    "            cleaned_content = clean_body_content(body_content)  # Clean the content\n",
    "\n",
    "            # Concatenate the cleaned content to the respective key in the dictionary\n",
    "            content_dict[page] += cleaned_content + \" \"  # Add a space between concatenated content\n",
    "\n",
    "    return content_dict\n",
    "    \n",
    "    \n",
    "\n",
    "def re_match_second_level_headlines(headline, domain, links_to_visit):\n",
    "    matching_urls = []\n",
    "    headline_url = urljoin(domain, headline)\n",
    "    matching_urls.extend([url for url in links_to_visit if url.startswith(headline_url)])\n",
    "    return matching_urls\n",
    "    \n",
    "    \n",
    "        refinement_prompt = (f'This is the output from an LLM that has been fed multiple chunks of information: {initial_result}.'\n",
    "                     'Please clean this output up by following these instructions carefully: \\n\\n'\n",
    "                     '1. Format: The output will likely be in multiple tables, please condense all output into one table. IMPORTANT: “Output the table as valid CSV with exactly the same number of columns for all rows (have fill value \"empty\" if there is not input for a given cell), no extra lines, and properly quoted cells if they contain commas. Use a semicolon to seperate cells instead of a comma”'\n",
    "                     '2. No duplication: The table you create should not contain duplicate rows, there should only be one row with relevant information for each e.g. product, location, customer etc'\n",
    "                     '3. If the tables you receive have multiple different categories (e.g. solutions, locations, customers) still include them into one table and add a columns called \"type\" to your table that captures this information'\n",
    "                     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_with_chatgpt(dom_chunks, parse_description):\n",
    "    \"\"\"\n",
    "    Sends chunks of website content to OpenAI's GPT API for parsing based on a description.\n",
    "    \"\"\"\n",
    "\n",
    "    template = (\n",
    "        f'You are tasked with extracting specific information from the following text content: {dom_chunks}.'\n",
    "        'Please follow these instructions carefully: \\n\\n'\n",
    "        f' 1. ** Extract Information** : Only extract the information that directly matches the provided description {parse_description}'\n",
    "        ' 2. ** No Extra Content** : Do not include any additional text, comments or explanations in your response.'\n",
    "        ' 3. ** Empty Response** : If no information matches the description return to an empty string'\n",
    "        ' 4. ** Direct Data** : Your response should contain only data that is explicitly requested, with no other text.'\n",
    "        ' 5. ** Format** : **Single Table**: Output exactly one table, no extra lines or text.'\n",
    "                        '  - Use semicolons (`;`) to separate columns.'\n",
    "                        '  - Properly quote cells with (\"\") if they contain semicolons or commas.'\n",
    "                        '  - If a cell has no data, use `empty`.'\n",
    "        )\n",
    "\n",
    "    # Loop through chunks and get responses\n",
    "    parsed_results = []\n",
    "    for i, chunk in enumerate(dom_chunks, start=1):\n",
    "        prompt = templa\n",
    "te.format(dom_chunks=chunk, parse_description=parse_description)\n",
    "        print(f\"Processing chunk {i}/{len(dom_chunks)}...\")\n",
    "\n",
    "        try:\n",
    "            response = run_chatgpt(prompt)\n",
    "            \n",
    "            if isinstance(response, list):\n",
    "                parsed_results.append(response[0].choices[0].message.content)\n",
    "            else:\n",
    "                parsed_results.append(response.choices[0].message.content)\n",
    "\n",
    "            # Extract the content of the assistant's reply\n",
    "            parsed_results.append(response[0].choices[0].message.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i}: {e}\")\n",
    "            parsed_results.append(\"\")  # Append an empty string if there's an error\n",
    "\n",
    "    final_result = []\n",
    "\n",
    "    initial_result = \"\\n\".join(parsed_results)\n",
    "\n",
    "    refinement_prompt = (f'This is the output from an LLM that has been fed multiple chunks of information: {initial_result}. Please merge them into a single CSV table with these requirements:'\n",
    "                        '1. **Single Table**: Output exactly one table, no extra lines or text.'\n",
    "                        '2. **Format**:'\n",
    "                        '   - Use semicolons (`;`) to separate columns.'\n",
    "                        '   - Properly quote cells with (\"\") if they contain semicolons or commas.'\n",
    "                        '   - If a cell has no data, use `empty`.'\n",
    "                        '3. **Columns**:'\n",
    "\n",
    "                        '   - All rows must have the same number of columns.'\n",
    "                        '   - No duplicate rows.'\n",
    "                        '   - If the data covers multiple categories (e.g. solutions, locations, customers), add a column named `\"Type\"` to capture the category.'\n",
    "                        '4. **No Duplicates**: Merge repeated rows or near-identical information into a single row.'\n",
    "                        '5. **Final Output**:'\n",
    "                        '   - No code fences or markdown.'\n",
    "                        '   - Strictly the CSV text (with one header row).')\n",
    "\n",
    "    if len(initial_result) > 12000:\n",
    "        dom_chunks = split_dom_content(initial_result)\n",
    "        for i, chunk in enumerate(dom_chunks,start=1):\n",
    "            prompt_02 = refinement_prompt.format(initial_result=chunk)\n",
    "            print(f\"Processing refinement chunk {i}/{len(dom_chunks)}...\")\n",
    "            try:\n",
    "                response = run_chatgpt(prompt_02)\n",
    "                if isinstance(response, list):\n",
    "                    final_result.append(response[0].choices[0].message.content)\n",
    "                else:\n",
    "                    final_result.append(response.choices[0].message.content)\n",
    "\n",
    "                # Extract the content of the assistant's reply\n",
    "                final_result.append(response[0].choices[0].message.content)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing refinement chunk {i}: {e}\")\n",
    "                final_result.append(\"\")  # Append an empty string if there's an error\n",
    "\n",
    "    else:\n",
    "\n",
    "        prompt_02 = refinement_prompt.format(initial_result=initial_result)\n",
    "        print(f\"Refining initial result...\")\n",
    "        try:\n",
    "            response = run_chatgpt(prompt_02)\n",
    "            if isinstance(response, list):\n",
    "                final_result.append(response[0].choices[0].message.content)\n",
    "            else:\n",
    "                final_result.append(response.choices[0].message.content)\n",
    "\n",
    "            # Extract the content of the assistant's reply\n",
    "            final_result.append(response[0].choices[0].message.content)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i}: {e}\")\n",
    "            final_result.append(\"\")  # Append an empty string if there's an error\n",
    "\n",
    "    return \"\\n\".join(final_result)\n",
    "                     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
